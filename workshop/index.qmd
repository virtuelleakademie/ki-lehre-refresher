---
title: "Antworten von Sprachmodellen kritisch prüfen"
subtitle: "Ein 40-Minuten Workshop zu Lateral Reading"
---

## Überblick

Dieser Workshop vermittelt eine praktische Technik zur Verifikation von Antworten aus Large Language Models (LLMs) wie Copilot, ChatGPT, Claude oder Gemini: **Lateral Reading**.

Du lernst durch eigene Experimente, wann und wie du Antworten von Sprachmodellen extern verifizierst.

## Lernziele

Nach diesem Workshop kannst du:

1. **Erkennen**, wann Antworten von Sprachmodellen externe Verifikation brauchen
2. **Verstehen**, warum Nachfragen beim Sprachmodell keine echte Prüfung ist
3. **Lateral Reading anwenden**, um Fakten zu überprüfen
4. **Vertrauenswürdige Quellen** für deinen Arbeitsbereich identifizieren

## Kurz-Check-in (1 Min)

- Wie stark stimmst du zu: "Antworten von Sprachmodellen kann ich ohne weitere Prüfung verwenden"?
- Wie sicher fühlst du dich darin, falsche Angaben von Sprachmodellen zu erkennen? (0--10)

Notiere deine beiden Antworten kurz (Zettel oder Notiz-App). Du brauchst sie beim Check-out. Keine Abgabe.

## Ablauf

| Phase | Dauer | Inhalt |
|-------|-------|--------|
| [Einstieg](framing/index.qmd) | 30 Sek | Direkt ins Experiment |
| [Teil 1](experiment-1/index.qmd) | 8 Min | Das Sprachmodell liegt falsch |
| [Teil 2](experiment-2/index.qmd) | 6 Min | Warum Nachfragen nicht hilft |
| [Teil 3](lateral-reading/index.qmd) | 22 Min | Lateral Reading üben |
| [Teil 4](entscheidungsregel/index.qmd) | 3.5 Min | Die Entscheidungsregel |

**Gesamtdauer:** 40 Minuten

## Was du brauchst

- Laptop oder Tablet mit Internetzugang
- Zugang zu einem Sprachmodell (Copilot, ChatGPT, Claude, oder Gemini)

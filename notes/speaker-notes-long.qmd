---
title: "Speaker Notes: KI in der Hochschulbildung"
subtitle: "Ausführliche Notizen mit Hintergrundinformationen"
author: "Dr. Andrew Ellis"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Teil 1: Was KI heute kann

## Titelfolie: Teil 1

**Sprechtext:**

Willkommen zum ersten Teil unserer Veranstaltung. Wir beginnen mit einer Bestandsaufnahme: Was kann KI heute tatsächlich leisten? Diese Frage ist wichtig, weil viele von uns vielleicht nicht täglich mit den neuesten Entwicklungen Schritt halten können.

::: {.callout-note}
## Hintergrund
Der erste Teil dient als Orientierung und gemeinsame Wissensbasis. Viele Teilnehmende haben unterschiedliche Erfahrungsstände mit KI. Dieser Einstieg soll alle auf denselben Stand bringen, bevor wir zu den pädagogischen Implikationen kommen.
:::

---

## Die zentrale Frage

**Sprechtext:**

Schauen wir uns an, was KI heute kann. Die linke Spalte zeigt beeindruckende Fähigkeiten: KI besteht Anwaltsprüfungen, schreibt und debuggt Code, kann Krankheiten diagnostizieren, wissenschaftliche Texte verfassen und komplexe Analysen durchführen.

Aber hier ist der Punkt, der uns heute beschäftigen wird. [KLICK]

Rechts sehen wir ein Zitat aus einer aktuellen Studie von Bastani und Kollegen: Studierende lösten 48% mehr Aufgaben MIT KI, aber schnitten 17% schlechter ab, wenn sie OHNE KI arbeiteten.

[KLICK]

Die Frage ist also nicht, OB KI hilft. Die Frage ist: WANN hilft sie, und WANN schadet sie? Das ist die Leitfrage für heute.

::: {.callout-note}
## Hintergrund zur Bastani-Studie
Die Studie von Bastani et al. (2025) "Generative AI Can Harm Learning" ist eine randomisierte kontrollierte Studie mit etwa 1000 türkischen Gymnasiasten. Sie ist eine der ersten rigorosen Studien, die sowohl die unmittelbaren Leistungsgewinne als auch die längerfristigen Lerneffekte von KI-Nutzung untersucht. Die Zahlen sind dramatisch: Die Produktivitätssteigerung während der Nutzung wird durch Lernverluste danach mehr als aufgehoben.
:::

---

## Wie LLMs Text produzieren

**Sprechtext:**

Bevor wir über pädagogische Implikationen sprechen, müssen wir verstehen, wie diese Systeme funktionieren.

[KLICK für jeden Punkt]

LLMs funktionieren über Next-Word-Prediction. Sie sagen voraus, welches Wort am wahrscheinlichsten als nächstes kommt. Sie wurden auf Milliarden von Textdokumenten trainiert und erkennen Muster in Sprache, Argumentation und Stil.

Eine hilfreiche Metapher: Stellen Sie sich eine extrem ausgeklügelte Autovervollständigung vor. Wie auf Ihrem Handy, nur millionenfach komplexer.

[KLICK]

Und hier ist der entscheidende Punkt: LLMs RUFEN kein Wissen ab wie eine Datenbank. Sie GENERIEREN Text basierend auf statistischen Mustern. Das hat wichtige Implikationen für die Zuverlässigkeit.

::: {.callout-note}
## Technischer Hintergrund
Large Language Models (LLMs) wie GPT-4 oder Claude sind neuronale Netzwerke mit Milliarden von Parametern. Sie lernen statistische Zusammenhänge zwischen Wörtern und Phrasen. Die "Next-Word-Prediction"-Aufgabe während des Trainings zwingt das Modell, implizit Weltwissen, Grammatik, Logik und sogar Reasoning zu lernen, um die nächste Token-Wahrscheinlichkeit korrekt vorherzusagen.

Wichtig: Dies bedeutet nicht, dass LLMs "nur" Statistik machen. Die emergenten Fähigkeiten sind real. Aber die Grundlage ist statistische Mustererkennung, nicht symbolische Wissensrepräsentation.
:::

---

## Wie "Denken" in KI funktioniert

**Sprechtext:**

Neuere Modelle können auch "denken", also Chain-of-Thought Reasoning durchführen.

Schauen wir uns das Beispiel an. Links: Ohne Reasoning fragt man "Was ist 17 mal 24?" und bekommt einfach "408" als Antwort.

Rechts: Mit Reasoning teilt das Modell das Problem auf. Es sagt: "Lass mich das aufteilen: 17 mal 20 ist 340, 17 mal 4 ist 68, zusammen 408."

[KLICK]

Der Kernpunkt: Das ist immer noch Mustererkennung, aber das Modell hat gelernt, über Denkschritte nachzudenken, nicht nur über Endergebnisse. Es generiert den Lösungsweg, nicht nur die Antwort.

::: {.callout-note}
## Hintergrund zu Chain-of-Thought
Chain-of-Thought (CoT) wurde 2022 von Wei et al. bei Google eingeführt. Die Entdeckung war, dass grosse Sprachmodelle deutlich besser bei komplexen Aufgaben werden, wenn man sie auffordert, "Schritt für Schritt" zu denken.

Moderne "Reasoning-Modelle" wie o1 oder Claude mit erweitertem Denken haben diese Fähigkeit internalisiert. Sie produzieren automatisch Zwischenschritte, bevor sie zur Antwort kommen. Dies verbessert die Leistung bei mathematischen, logischen und mehrstufigen Problemen erheblich.
:::

---

## Werkzeug-fähige Agenten

**Sprechtext:**

Die nächste Entwicklungsstufe sind Agenten. Die Formel ist einfach: LLM plus Werkzeuge ergibt einen Agenten.

[KLICK für jeden Punkt]

Agenten können: Websuche durchführen, Code ausführen, Dateien lesen und schreiben, Berechnungen anstellen, und APIs aufrufen.

[KLICK]

Die Konsequenz: Agenten können fast jede kognitive Aufgabe ausführen. Literaturrecherche? Kein Problem. Datenanalyse? Ja. Schreiben und Überarbeiten? Natürlich. Programmieren? Definitiv.

Das ist der Stand der Technik, den wir heute haben.

::: {.callout-note}
## Hintergrund zu KI-Agenten
Agenten sind die aktuelle Frontier der KI-Entwicklung. Im Gegensatz zu reinen Chat-Interfaces können Agenten:

1. **Planen**: Komplexe Aufgaben in Teilschritte zerlegen
2. **Ausführen**: Werkzeuge wie Browser, Code-Interpreter, Dateisysteme nutzen
3. **Reflektieren**: Ergebnisse bewerten und iterieren

Beispiele: Devin (Programmier-Agent), AutoGPT, Claude Computer Use. Diese Systeme können stundenlang autonom an Aufgaben arbeiten.

Für die Bildung bedeutet dies: Studierende können nicht nur Antworten bekommen, sondern ganze Aufgaben delegieren.
:::

---

## Diskussion 1

**Sprechtext:**

Nehmen wir uns fünf Minuten für eine kurze Diskussion.

Zwei Fragen für Sie: Erstens, welche KI-Fähigkeiten haben Sie überrascht? Was konnten diese Systeme, was Sie nicht erwartet hätten?

Und zweitens, wo haben Sie Studierende beim Einsatz von KI beobachtet? In welchen Kontexten nutzen sie diese Werkzeuge?

Sie haben fünf Minuten. Tauschen Sie sich mit Ihren Nachbarn aus.

[Timer läuft]

::: {.callout-note}
## Moderationshinweis
Diese erste Diskussion dient zwei Zwecken: Erstens aktiviert sie die Teilnehmenden nach dem Inputteil. Zweitens sammelt sie Praxiserfahrungen, die später aufgegriffen werden können.

Mögliche Nachfragen nach der Diskussion:
- "Was war das Überraschendste, was Sie gehört haben?"
- "Hat jemand problematische Nutzungsmuster beobachtet?"
:::

---

# Teil 2: Wie Expertise entsteht

## Titelfolie: Teil 2

**Sprechtext:**

Jetzt, da wir verstehen, was KI kann, müssen wir verstehen, wie Menschen Expertise entwickeln. Denn nur wenn wir den Lernprozess verstehen, können wir beurteilen, wie KI ihn beeinflusst.

::: {.callout-note}
## Hintergrund
Dieser Teil basiert auf etablierten Theorien der Kognitionspsychologie: ACT-R (Anderson), Cognitive Load Theory (Sweller), und Expertise-Forschung (Chi, Ericsson). Diese Theorien sind seit Jahrzehnten empirisch bestätigt und bilden die Grundlage für evidenzbasierte Instruktionsdesign.
:::

---

## Experten und Novizen sind grundlegend verschieden

**Sprechtext:**

Die erste wichtige Erkenntnis: Experten und Novizen unterscheiden sich nicht nur quantitativ, sondern qualitativ.

Es ist nicht nur "mehr Wissen". Es ist eine andere kognitive Architektur.

Das klassische Beispiel kommt aus der Schachforschung von de Groot und Chase & Simon. Wenn man Schachmeistern und Anfängern kurz eine Spielstellung zeigt und sie dann reproduzieren lässt, erinnern Meister viel mehr.

Aber hier ist der Clou: Das gilt nur für sinnvolle Spielstellungen. Bei zufällig platzierten Figuren sind Meister nicht besser als Anfänger.

Warum? Schachmeister sehen Muster und Strategien. Anfänger sehen einzelne Figuren auf Feldern. Die Experten haben ihr Wissen anders organisiert.

[KLICK]

Wie jemand einmal sagte: "Experts don't just know more; they organize knowledge differently."

::: {.callout-note}
## Hintergrund zur Schachforschung
Die Studien von de Groot (1965/1978) und Chase & Simon (1973) sind Klassiker der Kognitionspsychologie. De Groot zeigte ursprünglich, dass Grossmeister nicht tiefer rechnen als schwächere Spieler, sondern bessere Stellungen erkennen.

Chase & Simon quantifizierten dies: Meister erinnerten ca. 16 von 24 Figuren korrekt, Anfänger nur 4. Bei Zufallsstellungen waren beide Gruppen bei etwa 3-4 Figuren.

Die Schlussfolgerung: Expertise basiert auf "Chunks", also bedeutungsvollen Einheiten im Langzeitgedächtnis. Geschätzt haben Schachmeister etwa 50.000 solcher Chunks gespeichert.
:::

---

## Von schwachen zu starken Methoden

**Sprechtext:**

Die ACT-R Theorie von John Anderson beschreibt, wie der Übergang vom Novizen zum Experten funktioniert.

Links sehen wir die "schwachen Methoden" der Novizen: Mittel-Ziel-Analyse, also "was ist mein Ziel, wo bin ich, was könnte ich tun?". Versuch und Irrtum. Analogiebildung. Rückwärtsarbeiten vom Ziel.

Diese Methoden funktionieren, aber sie sind langsam und belasten das Arbeitsgedächtnis stark.

Rechts sehen wir die "starken Methoden" der Experten: Automatische Mustererkennung, prozeduralisiertes Wissen, direkte Lösungswege. Experten sehen ein Problem und wissen sofort, was zu tun ist.

[KLICK]

Entscheidend: Der Übergang von schwach zu stark erfordert umfangreiche Übung. Es gibt keine Abkürzung.

::: {.callout-note}
## Hintergrund zu ACT-R
ACT-R (Adaptive Control of Thought-Rational) ist eine der einflussreichsten kognitiven Architekturen. Sie unterscheidet zwischen:

- **Deklarativem Wissen**: Fakten, die bewusst abgerufen werden ("Die Hauptstadt von Frankreich ist Paris")
- **Prozeduralem Wissen**: Automatisierte Wenn-Dann-Regeln ("Wenn ich das Muster X sehe, dann tue Y")

Der Übergang von deklarativ zu prozedural ist die "Kompilation" von Wissen. Sie erfordert wiederholte Anwendung in variierenden Kontexten.

Wichtig: KI kann die Ergebnisse prozeduralen Wissens liefern, aber nicht den Kompilationsprozess im Lernenden auslösen.
:::

---

## Prozeduralisierung und Kompilation

**Sprechtext:**

Schauen wir uns den Prozess genauer an, am Beispiel des Autofahrens.

[KLICK für jeden Schritt]

Erstens: Deklaratives Wissen. "Man muss beim Autofahren die Kupplung treten, bevor man schaltet." Sie wissen es als Fakt.

Zweitens: Bewusste Schritte. Sie denken aktiv an jeden Schritt. "Okay, jetzt Kupplung... jetzt Gang rein... jetzt langsam loslassen..."

Drittens: Prozeduralisierung. Die einzelnen Schritte werden zu grösseren Einheiten zusammengefasst. Sie denken nicht mehr an die Kupplung, Sie denken "jetzt anfahren".

Viertens: Automatisierung. Sie schalten unbewusst, während Sie sich unterhalten oder an etwas anderes denken.

[KLICK]

Und hier ist der Schlüssel: Diese Transformation kann nicht übersprungen werden. Sie können nicht vom Wissen direkt zur Automatisierung springen.

::: {.callout-note}
## Hintergrund
Die Phasen entsprechen Fitts und Posners (1967) Modell des Fertigkeitserwerbs:

1. **Kognitive Phase**: Verstehen, was zu tun ist
2. **Assoziative Phase**: Verbinden von Hinweisen und Reaktionen
3. **Autonome Phase**: Automatische, flüssige Ausführung

Ericssons Forschung zu "Deliberate Practice" zeigt, dass ca. 10.000 Stunden gezielter Übung nötig sind für Expertise in komplexen Domänen. Diese Zahl ist oft missverstanden worden, aber der Kern bleibt: Expertise erfordert extensive, qualitativ hochwertige Übung.
:::

---

## Cognitive Load Theory: Die Grundlagen

**Sprechtext:**

Jetzt kommen wir zu einer der wichtigsten Theorien für das Instruktionsdesign: die Cognitive Load Theory von John Sweller.

Die Theorie unterscheidet zwei Gedächtnissysteme.

Links das Arbeitsgedächtnis: Es kann nur 4 plus/minus 1 Elemente gleichzeitig halten, für etwa 15 bis 30 Sekunden. Das Arbeitsgedächtnis ist der Engpass allen Lernens.

Rechts das Langzeitgedächtnis: Unbegrenzte Kapazität, dauerhafte Speicherung. Hier lebt Expertise.

[KLICK]

Die Kernaussage: Alles Lernen muss durch das Nadelöhr des Arbeitsgedächtnisses. Wenn wir dieses Nadelöhr verstopfen, kann kein Lernen stattfinden.

::: {.callout-note}
## Hintergrund zur Cognitive Load Theory
Die CLT wurde von John Sweller in den 1980ern entwickelt und ist eine der am besten empirisch gestützten Theorien der Instruktionspsychologie.

Die "Magische Zahl 7 plus/minus 2" von Miller (1956) wurde später auf etwa 4 Elemente korrigiert (Cowan, 2001). Aber: Durch "Chunking" können erfahrene Lernende mehr Information in weniger Einheiten organisieren.

Der aktuelle Stand (Sweller, 2024) betont den Unterschied zwischen biologisch primären (evolutionär erworbenen, wie Sprechen) und sekundären (kulturell entwickelten, wie Lesen) Fähigkeiten. Nur sekundäre Fähigkeiten unterliegen den Beschränkungen der CLT.
:::

---

## Drei Arten kognitiver Belastung

**Sprechtext:**

Die Theorie unterscheidet drei Arten kognitiver Belastung.

Intrinsische Belastung ist die inhärente Komplexität des Materials. Wenn Sie Quantenmechanik lernen, ist das komplex, unabhängig von der Didaktik. Diese Belastung kann nicht reduziert werden, ohne das Material zu vereinfachen.

Extrinsische Belastung entsteht durch schlecht gestaltete Instruktion. Unklare Erklärungen, ablenkende Animationen, unnötige Informationen. Diese Belastung sollte minimiert werden.

Lernförderliche oder "germane" Belastung ist die produktive Anstrengung für die Schemabildung. Das Ringen mit dem Material, das tatsächlich zu Lernen führt. Diese Belastung sollte erhalten bleiben.

Eine Anmerkung: Die Unterscheidung zwischen lernförderlicher und intrinsischer Belastung ist in der Literatur umstritten. Aber die praktische Implikation bleibt.

[KLICK]

Und hier ist die entscheidende Frage für uns: Welche Art der Belastung reduziert KI?

::: {.callout-note}
## Hintergrund zur Drei-Typen-Debatte
Die Unterscheidung von drei Belastungstypen wurde von Sweller und Kollegen später selbst kritisiert. Kalyuga (2011) und andere argumentieren, dass "germane load" konzeptuell schwer von intrinsischer Belastung zu trennen ist.

Die praktische Implikation bleibt jedoch: Manche Anstrengung ist produktiv (führt zu Schemabildung), manche ist unproduktiv (verschwendet kognitive Ressourcen). Die Frage bei KI ist: Reduziert sie die unproduktive oder die produktive Anstrengung?

Erste Evidenz (Bastani et al.) deutet darauf hin, dass KI beides reduziert, aber besonders die produktive Anstrengung.
:::

---

## Der Expertise-Umkehr-Effekt

**Sprechtext:**

Jetzt kommt einer der wichtigsten Befunde für unsere Diskussion: der Expertise-Umkehr-Effekt.

Schauen wir uns an, was die Forschung zeigt.

Links: Lernende mit geringen Vorkenntnissen profitieren von hoher Unterstützung. Die Effekte sind mittel bis gross. Das kennen wir, das macht Sinn.

Rechts: Lernende mit hohen Vorkenntnissen profitieren von NIEDRIGER Unterstützung. Und hier kehren sich die Effekte um. Was Novizen hilft, behindert Experten.

[KLICK]

Die Implikation ist fundamental: Dasselbe Werkzeug kann gegenteilige Effekte haben, abhängig vom Expertisegrad des Lernenden.

::: {.callout-note}
## Hintergrund zum Expertise-Umkehr-Effekt
Der Effekt wurde erstmals von Kalyuga et al. (1998) nachgewiesen. Die Meta-Analyse von Kalyuga (2009) zeigt:

- **Für Novizen**: Worked Examples sind effektiver als Problemlösen (d ≈ 0.8)
- **Für Experten**: Problemlösen ist effektiver als Worked Examples (d ≈ -0.3 bis -0.5)

Der Mechanismus: Experten haben bereits Schemata im Langzeitgedächtnis. Zusätzliche Instruktion ist redundant und muss integriert werden, was kognitive Ressourcen verschwendet.

Übertragen auf KI: Was für einen Experten eine hilfreiche Unterstützung ist (z.B. Boilerplate-Code generieren lassen), könnte für einen Novizen die Lernmöglichkeit eliminieren.
:::

---

## Explizite Instruktion: Was Lernen fördert

**Sprechtext:**

Wie sollte man dann unterrichten? Kirschner, Sweller und Clark haben 2006 einen einflussreichen Artikel dazu geschrieben.

[KLICK für jeden Punkt]

Das Problem mit minimaler Anleitung: Konstruktivistische, entdeckende Ansätze klingen attraktiv. "Lass die Studierenden selbst entdecken!"

Aber: Für Novizen ist minimale Anleitung weniger effektiv als explizite Instruktion. Warum?

Novizen haben keine Schemata im Langzeitgedächtnis. Ihr Arbeitsgedächtnis wird schnell überlastet, wenn sie ohne Orientierung suchen müssen.

[KLICK]

Die evidenzbasierte Alternative: Explizite Instruktion mit Worked Examples.

::: {.callout-note}
## Hintergrund zum Kirschner et al. Artikel
"Why Minimal Guidance During Instruction Does Not Work" (2006) ist einer der meistzitierten Artikel der Instruktionspsychologie. Er argumentiert gegen reine Entdeckungs-, Problem- und Inquiry-basierte Ansätze für Novizen.

Wichtig: Der Artikel behauptet nicht, dass Entdeckungslernen nie funktioniert. Er behauptet, dass es für Novizen in strukturierten Domänen weniger effektiv ist als gut gestaltete explizite Instruktion.

Kritik am Artikel: Hmelo-Silver et al. (2007) argumentieren, dass gutes Problem-Based Learning nicht "minimal guidance" ist, sondern "scaffolded guidance". Die Debatte geht weiter, aber der Kern bleibt: Unstrukturierte Exploration ist für Novizen ineffektiv.
:::

---

## Worked Examples: Ein robuster Befund

**Sprechtext:**

Was sind Worked Examples? Statt Probleme selbst lösen zu lassen, zeigen Sie ausgearbeitete Lösungswege.

Für Novizen reduziert das die extrinsische Belastung und lässt Kapazität für Schemabildung. Das ist einer der robustesten Befunde der Instruktionsforschung.

Aber Achtung: Worked Examples sind nicht dasselbe wie KI-generierte Lösungen.

[KLICK]

Der entscheidende Unterschied: Worked Examples sind didaktisch gestaltet. Sie bauen systematisch Komplexität auf. Sie sind sequenziert. Sie heben die wichtigen Schritte hervor.

KI-Antworten sind reaktiv und unsystematisch. Sie antworten auf die spezifische Frage, ohne pädagogischen Plan.

::: {.callout-note}
## Hintergrund zu Worked Examples
Die Forschung zu Worked Examples begann mit Sweller & Cooper (1985). Der Effekt ist robust repliziert: Novizen, die Worked Examples studieren, übertreffen Novizen, die äquivalente Probleme lösen.

Der Mechanismus: Beim Problemlösen ist das Arbeitsgedächtnis mit Mittel-Ziel-Analyse beschäftigt. Beim Studium von Worked Examples kann dieselbe kognitive Kapazität für Schemabildung genutzt werden.

Wichtig: Der Effekt gilt für Novizen. Mit zunehmender Expertise werden Worked Examples weniger effektiv (Expertise-Umkehr-Effekt) und können sogar kontraproduktiv werden.

KI-generierte Lösungen unterscheiden sich von Worked Examples:
1. Sie sind nicht für die Lernprogression sequenziert
2. Sie erklären oft das "Was" ohne das "Warum"
3. Sie sind reaktiv auf Fragen, nicht proaktiv didaktisch gestaltet
:::

---

## Von Worked Examples zu selbständigem Lösen

**Sprechtext:**

Wie kommt man von Worked Examples zu selbständigem Problemlösen? Durch Fading.

[KLICK für jeden Schritt]

Erstens zeigen Sie vollständige Worked Examples. Zweitens geben Sie Completion Problems, also teilweise gelöste Aufgaben, die vervollständigt werden müssen. Drittens reduzieren Sie die Vorgaben immer mehr. Viertens kommen Sie zu selbständigem Lösen ohne Unterstützung.

[KLICK]

Das verbindet sich mit dem Expertise-Umkehr-Effekt: Was Novizen hilft, kann Experten behindern. Daher brauchen wir dynamische Anpassung der Unterstützung.

::: {.callout-note}
## Hintergrund zu Fading
Renkl und Atkinson (2003) haben das Fading-Prinzip systematisch untersucht. Die Idee ist einfach: Anfangs volle Unterstützung, dann schrittweise Reduktion.

Typen von Fading:
- **Backward Fading**: Die letzten Schritte werden zuerst ausgeblendet
- **Forward Fading**: Die ersten Schritte werden zuerst ausgeblendet
- **Random Fading**: Schritte werden zufällig ausgeblendet

Die Evidenz favorisiert tendenziell Backward Fading, aber alle Fading-Ansätze sind effektiver als abrupter Übergang.

Verbindung zu KI: Ein gut gestaltetes KI-Tutoring-System müsste Fading implementieren. Die meisten tun das nicht.
:::

---

## Diskussion 2: Think-Pair-Share

**Sprechtext:**

Jetzt haben wir zehn Minuten für eine tiefere Reflexion im Think-Pair-Share Format.

In der "Think"-Phase, etwa zwei Minuten: Denken Sie an eine Fähigkeit, die Sie gemeistert haben. Nicht unbedingt akademisch. Vielleicht Fahrrad fahren, eine Sprache, ein Instrument, ein Sport. Wie hat sich der Lernprozess angefühlt?

In der "Pair"-Phase, etwa vier Minuten: Tauschen Sie sich mit Ihrem Nachbarn aus. Was haben Sie erlebt?

In der "Share"-Phase, etwa vier Minuten: Wie hat sich Ihr Denken verändert, als Sie Experte wurden? Was machen Sie jetzt automatisch, worüber Sie früher nachdenken mussten?

[Timer läuft]

::: {.callout-note}
## Moderationshinweis
Diese Übung macht die abstrakte Theorie persönlich erfahrbar. Die meisten Menschen können sich an den Übergang von bewusstem zu automatischem Handeln erinnern.

Mögliche Fragen für die Share-Phase:
- "Wer hat einen 'Aha-Moment' erlebt, wo plötzlich etwas 'klickte'?"
- "Wer erinnert sich an die Frustration der frühen Lernphase?"
- "Was wäre passiert, wenn Sie eine 'Abkürzung' gehabt hätten?"
:::

---

# Teil 3: Kritisches Denken braucht Fachwissen

## Titelfolie: Teil 3

**Sprechtext:**

Jetzt kommen wir zu einem Thema, das oft missverstanden wird: kritisches Denken. Wir hören viel davon, dass Studierende "kritisch mit KI umgehen" sollen. Aber was bedeutet das eigentlich?

::: {.callout-note}
## Hintergrund
Dieser Teil basiert hauptsächlich auf der Arbeit von Daniel Willingham, kognitiver Psychologe an der University of Virginia. Sein Artikel "Critical Thinking: Why Is It So Hard to Teach?" (2008) ist ein wichtiger Beitrag zur Debatte über übertragbare Fähigkeiten.
:::

---

## Die traditionelle Annahme

**Sprechtext:**

Schauen wir uns die traditionelle Annahme an.

[KLICK für jeden Punkt]

Kritisches Denken wird oft als übertragbare Fähigkeit betrachtet. Die "21st Century Skills"-Initiativen betonen das. KI-Kompetenz wird als generische Fertigkeit gesehen. Wir hören: "Wir müssen Studierende lehren, KI kritisch zu nutzen."

[KLICK]

Aber stimmt diese Annahme? Kann man "kritisches Denken" als allgemeine Fähigkeit lehren, die dann auf jeden Inhalt angewandt wird?

::: {.callout-note}
## Hintergrund zu 21st Century Skills
Die "21st Century Skills"-Bewegung (Partnership for 21st Century Learning, World Economic Forum) betont überfachliche Kompetenzen: kritisches Denken, Kreativität, Kommunikation, Kollaboration.

Die Kritik: Diese Fähigkeiten existieren nicht als kontextunabhängige Module. Man kann nicht "kritisches Denken im Allgemeinen" trainieren und dann auf beliebige Domänen anwenden.

Dies hat wichtige Implikationen für "KI-Kompetenz": Wenn kritisches Denken domänenspezifisch ist, dann ist auch die Fähigkeit, KI-Outputs zu bewerten, domänenspezifisch.
:::

---

## Willinghams Herausforderung

**Sprechtext:**

Hier ist Willinghams provokative These.

"Critical thinking is not a skill. There is not a set of critical thinking skills that can be acquired and deployed regardless of context."

[KLICK]

Denkprozesse sind eng mit Fachwissen verflochten.

Ich möchte eine Nuance hinzufügen: Willinghams Formulierung ist zugespitzt. Der Kern stimmt, aber Transfer ist nicht völlig unmöglich, nur viel schwieriger als oft angenommen.

::: {.callout-note}
## Hintergrund zu Willingham
Daniel Willingham ist Professor für Kognitionspsychologie und hat sich auf die Anwendung kognitionswissenschaftlicher Erkenntnisse auf Bildung spezialisiert. Sein Buch "Why Don't Students Like School?" ist ein Klassiker.

Seine Position ist nicht, dass kritisches Denken nicht existiert. Sie ist, dass es domänengebunden ist. Ein Historiker denkt kritisch über historische Quellen nach, aber diese Fähigkeit überträgt sich nicht automatisch auf medizinische Studien.

Dies ist konsistent mit der Expertise-Forschung: Expertise ist domänenspezifisch.
:::

---

## Evidenz für Domänenspezifität

**Sprechtext:**

Schauen wir uns die Evidenz an.

[KLICK für jeden Punkt]

Neurologen können Herzerkrankungen nicht gut diagnostizieren, obwohl sie medizinische Experten sind.

Fachredakteure, die Experten im Schreiben sind, können keine guten Zeitungsartikel in fremden Fachgebieten schreiben.

Und sogar Philosophen, die darauf trainiert sind, Argumente zu analysieren, werden von oberflächlichen Merkmalen beeinflusst, wenn sie Argumente in unbekannten Domänen bewerten.

[KLICK]

Wie Willingham sagt: "Abstract principles like 'look for hidden assumptions' won't help much in evaluating an argument about a topic you know little about."

::: {.callout-note}
## Hintergrund zur Domänenspezifität
Die Beispiele stammen aus verschiedenen Forschungsbereichen:

- **Medizin**: Norman et al. haben gezeigt, dass diagnostische Expertise eng an spezifische Krankheitsbilder gebunden ist
- **Schreiben**: Die Forschung von Hayes und Flower zeigt, dass Schreibexpertise domänengebundenes Wissen erfordert
- **Philosophie**: Studien von Schwitzgebel zeigen, dass auch trainierte Philosophen kontextabhängig urteilen

Der gemeinsame Nenner: Expertise ist an Inhalte gebunden, nicht an abstrakte Prozesse.
:::

---

## Angewendet auf KI-Bewertung

**Sprechtext:**

Was bedeutet das für die Bewertung von KI-Outputs?

Links: Ein Experte in Biomedizin erkennt, wenn ChatGPT bei Biochemie falsch liegt. Diese Person hat das Domänenwissen zur Bewertung.

Rechts: Ein Novize kann diese Bewertung nicht vornehmen, unabhängig davon, wie viel "kritisches Denken" er gelernt hat. Das fehlende Fachwissen verhindert die Evaluation.

[KLICK]

Was wie "kritisches Denken" aussieht, ist oft Domänenwissen in Aktion. Der Experte "denkt nicht kritischer", er weiss mehr.

::: {.callout-note}
## Hintergrund
Diese Überlegung hat praktische Implikationen für KI-Policies an Hochschulen. Wenn wir sagen "Studierende sollen KI kritisch nutzen", müssen wir fragen: Haben sie das Domänenwissen, um KI-Outputs in diesem Bereich zu bewerten?

Ein Mathematik-Studierender im ersten Semester kann nicht beurteilen, ob eine KI-generierte Lösung eines Analysis-Problems korrekt ist. Nicht weil kritisches Denken fehlt, sondern weil das mathematische Wissen fehlt.

Dies spricht für eine sequenzierte Einführung von KI: Erst das Fachwissen aufbauen, dann KI als Werkzeug einsetzen.
:::

---

## Was transferiert und was nicht

**Sprechtext:**

Ich möchte nicht zu nihilistisch sein. Einiges transferiert tatsächlich.

Links: Was teilweise transferiert. Metakognitive Strategien wie Planung, Überwachung des eigenen Verständnisses, Selbstregulation, und die Bereitschaft, Annahmen zu hinterfragen. Diese zeigen gewisse Generalisierung.

Rechts: Was kaum transferiert. Das Wissen, WAS plausibel ist. Das Wissen, WELCHE Quellen autoritativ sind. Das Erkennen von fachspezifischen Fehlern. Inhaltliche Bewertung erfordert Domänenwissen.

[KLICK]

Der Kernpunkt: Die Strategien kann man lehren. Aber ihre erfolgreiche Anwendung erfordert Fachwissen.

::: {.callout-note}
## Hintergrund zu Transfer
Die Transferforschung ist eines der ältesten Themen der Lernpsychologie. Die aktuelle Konsensposition:

- **Naher Transfer** (ähnliche Aufgaben, ähnliche Kontexte): relativ robust
- **Ferner Transfer** (verschiedene Domänen): schwer zu erreichen, aber nicht unmöglich

Metakognitive Strategien wie Selbstüberwachung ("Verstehe ich das wirklich?") zeigen etwas breiteren Transfer, aber nur wenn sie explizit trainiert und über Domänen hinweg geübt werden.

Die praktische Implikation: Wir können Studierenden beibringen, KI-Outputs zu hinterfragen. Aber ob sie Fehler erkennen, hängt von ihrem Fachwissen ab.
:::

---

## Die zentrale Implikation

**Sprechtext:**

Hier ist die zentrale Implikation für unsere Diskussion über KI.

Erstens: Studierende, die "mit hohem kritischem Denken" von KI profitieren, haben wahrscheinlich mehr Domänenexpertise. Es ist nicht kritisches Denken, es ist Fachwissen.

Zweitens: Die beste Vorbereitung für kritische KI-Nutzung ist tiefes Fachlernen. Nicht "KI-Kompetenz-Kurse", sondern Fachkompetenz.

Drittens: Generische "KI-Kompetenz" kann Fachwissen ergänzen, aber nicht ersetzen.

::: {.callout-note}
## Hintergrund
Diese Schlussfolgerung hat Implikationen für Curriculum-Design:

1. "KI-Kompetenz" als eigenständiges Fach ist begrenzt wirksam
2. KI-Integration muss fachspezifisch geschehen
3. Grundlagen-Fachkompetenz muss vor KI-Nutzung aufgebaut werden

Dies widerspricht dem Trend zu generischen "Digital Literacy" oder "AI Literacy" Kursen. Diese können Bewusstsein schaffen, aber nicht die Fähigkeit zur inhaltlichen Bewertung.
:::

---

## Diskussion 3

**Sprechtext:**

Nehmen wir uns fünf Minuten für eine kurze Diskussion.

Zwei Fragen: Hast du beobachtet, dass Studierende KI-Outputs in deinem Fach nicht bewerten können? Welches Domänenwissen wäre nötig?

Tausche dich mit deinen Nachbarn aus.

[Timer läuft]

::: {.callout-note}
## Moderationshinweis
Diese Diskussion macht die abstrakte Theorie konkret anwendbar. Die Teilnehmenden sollen über ihr eigenes Fach nachdenken.

Mögliche Nachfragen:
- "Welche typischen Fehler macht KI in Ihrem Fachgebiet?"
- "Wie würden Sie erkennen, ob ein Studierender einen KI-Output wirklich verstanden hat?"
:::

---

# Pause

**Sprechtext:**

Wir machen jetzt eine 15-minütige Pause. Bitte seien Sie pünktlich zurück.

[Timer läuft]

---

# Teil 4: Das Produktivitäts-Lern-Paradox

## Titelfolie: Teil 4

**Sprechtext:**

Willkommen zurück. Jetzt kommen wir zum Kern unserer Diskussion: das Produktivitäts-Lern-Paradox.

::: {.callout-note}
## Hintergrund
Dieser Teil integriert die vorangegangenen theoretischen Grundlagen mit empirischen Befunden zur KI-Nutzung. Die zentrale These ist, dass die Eigenschaften, die KI für Produktivität nützlich machen, sie für Lernen potenziell schädlich machen.
:::

---

## Das zentrale Paradox

**Sprechtext:**

Hier ist das zentrale Paradox, formuliert von Jose und Kollegen:

"Learning and task completion are not synonymous."

Lernen und Aufgabenerledigung sind nicht dasselbe.

[KLICK]

Links: KI verbessert nachweislich die Aufgabenleistung, die Geschwindigkeit und die Output-Qualität. Das ist gut dokumentiert.

Rechts: Aber Aufgabenleistung ist nicht gleich Lernen. Produktivität ist nicht gleich Kompetenzaufbau.

Das ist das Paradox, mit dem wir uns auseinandersetzen müssen.

::: {.callout-note}
## Hintergrund
Jose et al. (2025) "The Era of Outsourcing Cognition: A Psychological Analysis" bietet einen konzeptuellen Rahmen für die Analyse kognitiver Auslagerung an KI.

Die Unterscheidung zwischen Performance und Learning ist fundamental in der Lernpsychologie:
- **Performance**: Was jemand gerade tun kann
- **Learning**: Dauerhafte Veränderung in Wissen oder Fähigkeiten

KI verbessert Performance, aber ob sie Learning verbessert, ist empirisch offen.
:::

---

## Die Bastani-Studie: Mathematik

**Sprechtext:**

Schauen wir uns die Bastani-Studie genauer an. Es ist eine randomisierte kontrollierte Studie mit etwa 1000 türkischen Gymnasiasten, die GPT-4 Zugang während des Übens erhielten.

Wichtig: Dies ist eine Einzelstudie, Replikation steht noch aus.

[KLICK]

Die Ergebnisse: Mit KI lösten die Studierenden 48% mehr Aufgaben korrekt. Mit einer strukturierten "GPT Tutor"-Intervention sogar 127% mehr.

Aber: Ohne KI, bei einem späteren Test, schnitten sie 17% schlechter ab als die Kontrollgruppe, die nie KI hatte.

[KLICK]

Das Zitat der Autoren: "Students attempt to use GPT-4 as a 'crutch' during practice sessions, and when successful, perform worse on their own."

[KLICK]

Eine wichtige Nuance: Diese Ergebnisse gelten hauptsächlich für KI als Antwortgeber. Der "GPT Tutor", der pädagogisch strukturiert war, zeigte bessere Langzeiteffekte. Die Gestaltung der KI-Nutzung macht einen Unterschied.

::: {.callout-note}
## Hintergrund zur Bastani-Studie
Details der Studie:
- **Stichprobe**: ~1000 türkische Gymnasiasten
- **Design**: RCT mit mehreren Bedingungen
- **Intervention**: Zugang zu GPT-4 während Mathe-Übungen
- **Outcome**: Leistung in Tests ohne KI-Zugang

Wichtige methodische Punkte:
- Die Kontrollgruppe hatte keinen KI-Zugang (nicht Placebo)
- Die Studie lief über mehrere Wochen
- Der "GPT Tutor" war eine modifizierte Version mit pädagogischen Constraints

Limitationen:
- Einzelne Studie, noch nicht repliziert
- Spezifische Population (türkische Gymnasiasten)
- Spezifische Domäne (Mathematik)
:::

---

## Desirable Difficulties: Warum Anstrengung nötig ist

**Sprechtext:**

Um zu verstehen, warum das so ist, müssen wir über "Desirable Difficulties" sprechen. Das ist ein Konzept von Robert Bjork.

Das Zitat fasst es zusammen: "Conditions that slow the rate of apparent learning often optimize long-term retention and transfer."

Bedingungen, die das scheinbare Lernen verlangsamen, optimieren oft die Langzeitspeicherung und den Transfer.

[KLICK]

Es gibt vier bewährte "erwünschte Schwierigkeiten":

Variation der Lernbedingungen. Interleaving, also das Mischen von Aufgabentypen. Spacing, verteiltes statt massiertes Lernen. Und Retrieval Practice, Abrufen statt Wiederlesen.

[KLICK]

Der Mechanismus: Sofortiger KI-Zugang kann Abrufversuche kurzschliessen, bevor sie Gedächtnisspuren stärken können.

::: {.callout-note}
## Hintergrund zu Desirable Difficulties
Robert und Elizabeth Bjork haben dieses Konzept über Jahrzehnte entwickelt. Die Kernidee: Oberflächliche Indikatoren von Lernerfolg (schnelle Antworten, hohe Übungsleistung) sind oft negativ korreliert mit Langzeitlernen.

Die vier Schwierigkeiten:
1. **Variation**: Lernen in verschiedenen Kontexten erhöht Transfer
2. **Interleaving**: Gemischtes Üben ist kurzfristig schwieriger, langfristig besser
3. **Spacing**: Verteilte Wiederholung schlägt massiertes Üben
4. **Retrieval Practice**: Selbst-Testen ist effektiver als Wiederlesen

Alle diese Prinzipien werden potenziell durch sofortigen KI-Zugang untergraben.
:::

---

## Der Generierungseffekt

**Sprechtext:**

Ein verwandtes Phänomen ist der Generierungseffekt, ursprünglich von Slamecka und Graf beschrieben.

Selbst generierte Information wird besser behalten als passiv erhaltene.

Links: Wenn Sie selbst eine Antwort generieren, aktiviert das breite neuronale Netzwerke und führt zu besserer Langzeitspeicherung.

Rechts: Passiv erhaltene Information wird weniger tief verarbeitet und schwächer erinnert.

[KLICK]

Die Implikation: Wenn KI generiert, was Studierende selbst produzieren sollten, wird der Generierungseffekt eliminiert.

::: {.callout-note}
## Hintergrund zum Generierungseffekt
Der Generierungseffekt wurde 1978 von Slamecka & Graf entdeckt. Klassisches Paradigma:
- **Generate-Bedingung**: "hot - c__d" (Antwort: cold)
- **Read-Bedingung**: "hot - cold"

Ergebnis: Generierte Wörter werden besser erinnert.

Der Effekt ist robust repliziert und erklärt, warum aktives Lernen (Selbst-Erklären, Selbst-Testen) effektiver ist als passives (Lesen, Zuhören).

Direkte Implikation für KI: Wenn Studierende KI nach Antworten fragen, statt selbst zu generieren, verlieren sie den Generierungsvorteil.
:::

---

## Die Scaffolding-Hypothese

**Sprechtext:**

Jetzt eine Hypothese, die ich mit Vorsicht präsentieren möchte: die Scaffolding-Hypothese.

[KLICK für jeden Punkt]

Grundfertigkeiten wie Schreiben, Rechnen und Lesen sind nicht nur Fertigkeiten. Sie sind Prozesse, die kognitive Architektur aufbauen.

Schreiben zum Beispiel ist ein epistemisches Werkzeug: Gedanken entwickeln sich DURCH das Schreiben, nicht vor dem Schreiben.

Wenn diese Prozesse übersprungen werden, entstehen höhere Fähigkeiten möglicherweise nicht.

[KLICK]

Wichtig: Dies ist eine Hypothese, keine gesicherte Erkenntnis. Wir haben keine Längsschnittstudien, die zeigen, dass übersprungene kognitive Entwicklungsphasen irreversible Defizite verursachen.

[KLICK]

Eine wichtige Unterscheidung: Fertigkeitsatrophie bedeutet, dass eine vorhandene Fähigkeit verkümmert. Entwicklungsbeeinträchtigung bedeutet, dass eine Fähigkeit nie entsteht. Das sind verschiedene Phänomene.

::: {.callout-note}
## Hintergrund zur Scaffolding-Hypothese
Diese Hypothese ist spekulativer als die vorherigen Punkte. Sie basiert auf:

1. **Writing-to-learn Forschung**: Schreiben ist nicht nur Kommunikation, sondern Denkwerkzeug (Emig, 1977; Langer & Applebee, 1987)
2. **Entwicklungspsychologie**: Kognitive Fähigkeiten bauen aufeinander auf (Piaget, Vygotsky)
3. **Neuroplastizität**: Das Gehirn formt sich durch Erfahrung

Die Unsicherheit: Wir wissen nicht, ob es "sensible Perioden" für den Erwerb von Fähigkeiten wie wissenschaftlichem Schreiben gibt. Die Analogie zu Spracherwerb ist suggestiv, aber nicht bewiesen.

Daher die vorsichtige Formulierung als "Hypothese".
:::

---

## Historische Analogien

**Sprechtext:**

Schauen wir uns einige historische Analogien an.

GPS und räumliches Gedächtnis: Eine longitudinale Studie von Dahmani und Bhola über drei Jahre zeigt, dass stärkere GPS-Nutzung mit steilerem Rückgang des räumlichen Gedächtnisses korreliert. Die zeitliche Abfolge ist konsistent mit kausaler Interpretation, aber ungemessene Konfundierende sind nicht ausgeschlossen.

[KLICK]

Konzeptuelles Verständnis: Auch Erwachsene mit Rechenhilfen zeigen Lücken im konzeptuellen Verständnis von Dezimalzahlen. Werkzeugnutzung ersetzt kein Grundverständnis.

[KLICK]

Der Google-Effekt von Sparrow und Kollegen: Wir erinnern weniger, wenn wir erwarten, dass Information verfügbar ist. Menschen erinnern WO Information ist, nicht WAS sie ist.

::: {.callout-note}
## Hintergrund zu den historischen Analogien

**GPS-Studie (Dahmani & Bhola, 2020)**:
- 103 Teilnehmer über 3 Jahre
- GPS-Nutzung und räumliches Gedächtnis wurden wiederholt gemessen
- Ergebnis: GPS-Nutzung sagte Rückgang des räumlichen Gedächtnisses vorher, nicht umgekehrt
- Limitation: Korrelationsstudie, keine Randomisierung möglich

**Dezimalzahlen-Studie (Lortie-Forgues et al., 2017)**:
- Erwachsene und Kinder wurden zu konzeptuellem Verständnis von Dezimalzahlen befragt
- Viele Erwachsene können korrekt mit Dezimalzahlen rechnen, verstehen sie aber nicht konzeptuell
- Implikation: Prozedurales Können garantiert nicht konzeptuelles Verständnis

**Google-Effekt (Sparrow et al., 2011)**:
- Klassische Studie zur "kognitiven Auslagerung"
- Wenn Menschen wissen, dass Information digital verfügbar ist, speichern sie sie weniger
- Implikation: Unser Gedächtnis passt sich an verfügbare Werkzeuge an
:::

---

## Grenzen historischer Analogien

**Sprechtext:**

Aber ich muss auch die Grenzen dieser Analogien aufzeigen.

[KLICK für jeden Punkt]

GPS, Taschenrechner, Google: Das sind jeweils spezifische, enge kognitive Funktionen. Navigation, Berechnung, Informationsabruf.

Generative KI kann fast jede kognitive Aufgabe übernehmen. Schreiben, Argumentieren, Analysieren, Synthetisieren, Bewerten.

Die Breite ist beispiellos.

[KLICK]

Aber: Die historischen Bedenken hatten oft Berechtigung. GPS beeinflusst TATSÄCHLICH räumliche Kognition. Taschenrechner haben den Mathematikunterricht verändert.

Die Frage ist nicht OB Einfluss, sondern WIE VIEL und WIE.

::: {.callout-note}
## Hintergrund
Die Analogie-Kritik hat zwei Seiten:

**Pro Analogie**:
- Die kognitiven Mechanismen (Auslagerung, reduzierte Übung) sind dieselben
- Historische Technologien haben kognitive Effekte gehabt

**Contra Analogie**:
- Die Breite ist beispiellos
- KI kann nicht nur einzelne Aufgaben, sondern den gesamten kognitiven Workflow übernehmen
- Die Geschwindigkeit der Adoption ist höher

Die ehrliche Antwort: Wir wissen nicht, ob KI qualitativ anders ist oder nur quantitativ extremer.
:::

---

## Der EdTech-Hype-Zyklus

**Sprechtext:**

Hier eine Warnung aus der Geschichte der Bildungstechnologie, basierend auf Justin Reichs Buch.

[KLICK für jede Technologie]

Radio sollte die besten Vorlesungen in jedes Klassenzimmer bringen. Fernsehen sollte Lernen revolutionieren. Computer sollten Unterricht personalisieren. MOOCs sollten Elite-Bildung demokratisieren.

[KLICK]

Jede Technologie fand eine Nische, aber keine erfüllte die transformativen Versprechen.

[KLICK]

Was macht KI anders? Die Breite ist beispiellos. Aber dieselben strukturellen Kräfte könnten wirken.

::: {.callout-note}
## Hintergrund zum EdTech-Hype-Zyklus
Justin Reichs "Failure to Disrupt" (2020) dokumentiert vier wiederkehrende Muster:

1. **Enthusiasmus**: Neue Technologie wird als transformativ angekündigt
2. **Pilotprojekte**: Erste Erfolge unter idealen Bedingungen
3. **Skalierungsprobleme**: Bei breiter Einführung treten Probleme auf
4. **Nischenfindung**: Technologie findet begrenzten, aber stabilen Einsatz

Wichtig: Reich argumentiert nicht gegen Technologie. Er argumentiert gegen naive Techno-Utopie und für realistische Erwartungen.

Die Frage für KI: Wird KI diesem Muster folgen, oder ist sie qualitativ anders?
:::

---

## "Das haben sie über das Schreiben auch gesagt"

**Sprechtext:**

Ein häufiger Einwand: "Das haben sie über das Schreiben auch gesagt." Sokrates warnte im Phaidros, dass Schrift das Gedächtnis schwächen würde.

[KLICK]

Drei Antworten darauf:

Erstens: Schrift HAT Kognition tiefgreifend verändert. Wir denken anders als orale Kulturen.

Zweitens: Einige Bedenken waren berechtigt. Mündliche Gedächtnistraditionen SIND zurückgegangen. Wer kann heute noch das Gilgamesch-Epos auswendig rezitieren?

Drittens: Die Schriftkultur entwickelte sich über Jahrhunderte. Die KI-Integration geschieht in Jahren. Die Adaptionszeit ist völlig anders.

::: {.callout-note}
## Hintergrund zur Sokrates-Analogie
Platons Phaidros (ca. 370 v. Chr.) enthält Sokrates' berühmte Kritik an der Schrift:
- Schrift werde "Vergesslichkeit erzeugen"
- Menschen werden "mit dem Schein von Weisheit prahlen, nicht mit wirklicher Weisheit"

Die Ironie: Wir kennen diese Kritik nur, weil Platon sie aufschrieb.

Die differenzierte Antwort:
1. Sokrates hatte teilweise recht: Schrift veränderte das Gedächtnis
2. Aber Schrift ermöglichte auch neue kognitive Fähigkeiten
3. Der Übergang dauerte Jahrhunderte, nicht Jahre

Die Analogie mahnt zur Vorsicht bei beiden Extremen: weder naive Techno-Optimismus noch kategorische Ablehnung.
:::

---

## Warum Experten profitieren, Lernende nicht

**Sprechtext:**

Hier kommen alle unsere Stränge zusammen.

Links: Experten können von KI profitieren. Sie können Routine-Aufgaben sicher auslagern, weil sie wissen, was Routine ist. Sie können höheres Denken aufrechterhalten. Sie können KI-Outputs bewerten. Und sie haben Grundfähigkeiten, die nicht mehr verkümmern können.

Rechts: Lernenden fehlt all das. Ihnen fehlt das Wissen zur Bewertung. Ihnen fehlen etablierte Grundfähigkeiten. Ihnen fehlt metakognitive Kontrolle.

Das Risiko ist "fliessende Inkompetenz": anspruchsvoll wirkende Outputs ohne zugrundeliegendes Verständnis.

[KLICK]

Dasselbe Werkzeug, fundamental unterschiedliche Auswirkungen. Das ist der Expertise-Umkehr-Effekt, angewandt auf KI.

::: {.callout-note}
## Hintergrund zu "Fluent Incompetence"
Der Begriff stammt aus der Mathematikdidaktik und beschreibt Studierende, die korrekte Prozeduren ausführen können, ohne konzeptuelles Verständnis.

Übertragen auf KI: Studierende können hochwertig aussehende Outputs produzieren (mit KI-Hilfe), ohne das zugrundeliegende Wissen zu haben. Dies kann:
- Für die Studierenden selbst täuschend sein
- Für Lehrende schwer zu erkennen sein
- Im späteren Berufsleben problematisch werden

Das ist das Kern-Risiko: Nicht dass KI schlechte Outputs produziert, sondern dass sie gute Outputs produziert, die über fehlendes Lernen hinwegtäuschen.
:::

---

## Diskussion 4: Think-Pair-Share

**Sprechtext:**

Jetzt eine längere Reflexion von 15 Minuten.

Think-Phase: Wo könnte KI-Unterstützung in deinem Fach die produktive Anstrengung eliminieren, die Lernen ermöglicht?

Pair-Phase: Diskutiere mit deinem Nachbarn.

Share-Phase: Welche Grundfähigkeiten könnten bei KI-Nutzung verkümmern?

[Timer läuft]

::: {.callout-note}
## Moderationshinweis
Diese Diskussion ist die längste und wichtigste. Sie soll die Teilnehmenden dazu bringen, die theoretischen Konzepte auf ihr eigenes Fach anzuwenden.

Mögliche Nachfragen:
- "Wer hat konkrete Beispiele für 'produktive Anstrengung' in seinem Fach?"
- "Wo ist die Grenze zwischen hilfreicher Unterstützung und schädlicher Abkürzung?"
- "Wie würden Sie es erkennen, wenn Studierende Grundfähigkeiten nicht entwickeln?"
:::

---

# Exkurs: "Sokratisches Fragen" in KI-Tutoren

## Titelfolie: Exkurs

**Sprechtext:**

Bevor wir zur Synthese kommen, ein kurzer Exkurs zu einem Thema, das viel diskutiert wird: sokratisches Fragen in KI-Tutoren. Viele EdTech-Unternehmen versprechen genau das. Schauen wir uns an, was die Evidenz sagt.

::: {.callout-note}
## Hintergrund
Dieser Exkurs adressiert einen der häufigsten Gegenargumente: "Aber KI kann doch als sokratischer Tutor fungieren!" Die Antwort erfordert eine differenzierte Betrachtung von Tutoring-Forschung, Implementierungsherausforderungen und aktueller Evidenzlage.
:::

---

## Der Hype um den digitalen Sokrates

**Sprechtext:**

[KLICK für jeden Punkt]

EdTech verspricht: KI-Tutoren mit sokratischer Methode. Das klingt attraktiv.

Die Grundlage ist oft Blooms "Two Sigma Problem": 1:1-Tutoring erzielt angeblich zwei Standardabweichungen Verbesserung gegenüber Klassenunterricht.

Das Versprechen: Demokratisierung personalisierter Bildung durch KI. Jeder Studierende bekommt einen persönlichen Tutor.

[KLICK]

Aber die Begeisterung übersteigt die Evidenz erheblich.

::: {.callout-note}
## Hintergrund zu Blooms Two Sigma Problem
Benjamin Blooms "2 Sigma Problem" (1984) ist einer der meistzitierten Befunde der Bildungsforschung. Bloom behauptete, dass 1:1-Tutoring zu d = 2.0 Verbesserung führt.

Aber: Spätere Meta-Analysen (VanLehn, 2011) fanden kleinere Effekte (d ≈ 0.8). Der "2 Sigma"-Effekt ist wahrscheinlich überschätzt.

Trotzdem: Auch d = 0.8 ist ein grosser Effekt. Die Frage ist, ob KI-Tutoring diesen Effekt replizieren kann.
:::

---

## Warum Fragen theoretisch helfen könnten

**Sprechtext:**

Warum SOLLTE sokratisches Fragen funktionieren?

Links der Generierungseffekt: Selbst erzeugte Antworten werden besser behalten als passiv erhaltene. Das haben wir schon besprochen.

Rechts der Selbsterklärungseffekt: Wenn Lernende Konzepte erklären müssen, fördert das tiefere Verarbeitung und deckt Wissenslücken auf.

[KLICK]

Aber: Diese Effekte erfordern, dass Lernende genug Vorwissen haben, um sinnvolle Antworten zu generieren. Wenn die Wissensbasis fehlt, generieren sie nichts Sinnvolles.

::: {.callout-note}
## Hintergrund zum Selbsterklärungseffekt
Chi et al. (1994) zeigten, dass Studierende, die aufgefordert werden, Lösungsschritte zu erklären, besser lernen als Studierende, die nur lesen.

Der Mechanismus: Selbsterklärung zwingt zur aktiven Verarbeitung und macht Wissenslücken sichtbar.

Verbindung zu sokratischem Fragen: Gute sokratische Fragen sollen Selbsterklärung auslösen.

Das Problem: Wenn die Wissensbasis fehlt, führen Fragen zu Frustration, nicht zu Einsicht.
:::

---

## Was die Evidenz tatsächlich zeigt

**Sprechtext:**

Schauen wir uns an, was die Meta-Analysen zeigen.

[KLICK für jeden Punkt]

VanLehn analysierte 2011 die Tutoring-Forschung. Menschliche Tutoren erreichen d = 0.79, nicht 2.0 wie Bloom behauptete. Intelligente Tutorsysteme erreichen d = 0.76, vergleichbar.

Aber: Schritt-für-Schritt-Feedback war wesentlich für den Effekt. Die spezifische Rolle des sokratischen Dialogs ist schwer zu isolieren.

[KLICK]

Zu LLM-basierten sokratischen Tutoren: Keine gut kontrollierten RCTs vorhanden. Die Evidenz besteht hauptsächlich aus Zufriedenheitsumfragen und Vergleichen mit "kein Tutoring".

::: {.callout-note}
## Hintergrund zur VanLehn Meta-Analyse
VanLehn (2011) verglich:
- Menschliches Tutoring: d = 0.79
- Intelligente Tutorsysteme (ITS): d = 0.76
- Unstrukturierte Lernumgebungen: d = 0.31

Die Schlussfolgerung: ITS können menschliches Tutoring fast erreichen, aber beide sind weit von d = 2.0 entfernt.

Wichtig: Die effektiven ITS waren sorgfältig gestaltet, oft domänenspezifisch, und enthielten strukturiertes Feedback. Einfaches "Fragen stellen" war nicht ausreichend.
:::

---

## Das Diagnose-Problem

**Sprechtext:**

Hier ist ein fundamentales Problem: Effektives sokratisches Fragen erfordert Diagnose.

[KLICK für jeden Punkt]

Sie müssen genau einschätzen, was der Lernende aktuell weiss. Sie müssen verschiedene Fehlertypen unterscheiden. Sie müssen die Fragen an den spezifischen Lernenden anpassen.

[KLICK]

Nehmen wir ein Beispiel: Ein Studierender schreibt "1/2 + 1/3 = 2/5". Was bedeutet das?

Es könnte ein prozeduraler Fehler sein, Zähler und Nenner wurden addiert. Es könnte ein konzeptueller Fehler sein, die Person versteht Brüche nicht. Es könnte ein Flüchtigkeitsfehler sein.

[KLICK]

KI-Systeme können diese Unterscheidung nicht zuverlässig treffen. Und die richtige sokratische Frage hängt davon ab, welcher Fehlertyp vorliegt.

::: {.callout-note}
## Hintergrund zum Diagnose-Problem
Die Unterscheidung verschiedener Fehlertypen ist ein Kern-Problem der Instruktion:

- **Prozeduraler Fehler**: Der Algorithmus ist falsch verstanden
- **Konzeptueller Fehler**: Das zugrundeliegende Konzept fehlt
- **Performanz-Fehler**: Wissen ist vorhanden, aber nicht abgerufen

Die richtige Intervention hängt vom Fehlertyp ab:
- Prozedural: Richtige Prozedur zeigen
- Konzeptuell: Konzept aufbauen
- Performanz: Erinnerungshilfen

KI-Systeme neigen dazu, alle Fehler gleich zu behandeln, typischerweise durch Wiederholung der Erklärung.
:::

---

## Weitere Implementierungsherausforderungen

**Sprechtext:**

Es gibt weitere Implementierungsherausforderungen.

Links: Fragesequenzierung. Sokratischer Dialog ist kontingent. Jede Frage baut auf vorherigen Antworten auf. Es gibt einen pädagogischen Plan. LLMs generieren Token für Token, sie planen keine pädagogischen Sequenzen.

Rechts: Feedback-Timing. Wann korrigieren Sie, und wann fragen Sie weiter? Das hängt vom Lernenden und vom Fehlertyp ab. KI kann das nicht zuverlässig beurteilen.

[KLICK]

Und es gibt eine dokumentierte Tendenz von LLMs: Sie sind zu nachgiebig. Sie akzeptieren oft falsche Antworten als "interessante Perspektiven". Sie vermeiden produktives Unbehagen, das für Lernen nötig sein kann.

::: {.callout-note}
## Hintergrund zur Sycophancy
"Sycophancy" ist ein bekanntes Problem bei LLMs: Sie tendieren dazu, dem Nutzer zuzustimmen, auch wenn dieser falsch liegt.

Für pädagogische Anwendungen ist das problematisch:
- Fehler werden nicht klar korrigiert
- Studierende erhalten falsches positives Feedback
- Das "produktive Unbehagen" des Lernens wird vermieden

Neuere Modelle werden darauf trainiert, weniger sycophant zu sein, aber das Problem ist nicht gelöst.
:::

---

## Praktische Empfehlungen

**Sprechtext:**

Was bedeutet das praktisch?

[KLICK für jede Empfehlung]

Erstens: Evidenz verlangen. Peer-Review-Studien mit Lernoutcomes, nicht nur Zufriedenheitsumfragen.

Zweitens: Die Diagnose-Fähigkeit prüfen. Kann das System verschiedene Fehlertypen unterscheiden?

Drittens: Opportunitätskosten bedenken. Ist KI-Tutoring besser als Alternativen wie Peer-Tutoring oder strukturierte Übung?

Viertens: Mit strukturierten Domänen beginnen. Mathematik vor Literaturanalyse, wo richtig und falsch klarer sind.

Fünftens: Auf unbeabsichtigte Folgen achten. Gaming des Systems, Abhängigkeit von externen Prompts.

::: {.callout-note}
## Hintergrund
Diese Empfehlungen sind konservativ, aber empirisch begründet:

1. **Evidenz**: Die EdTech-Geschichte ist voll von Produkten, die Wirksamkeit behaupteten ohne Evidenz
2. **Diagnose**: Ohne gute Diagnose ist adaptives Tutoring nicht möglich
3. **Opportunitätskosten**: Zeit für KI-Tutoring ist Zeit, die nicht für anderes genutzt wird
4. **Strukturierte Domänen**: Mathematik hat klare Fehlerkriterien; Literatur nicht
5. **Gaming**: Studierende lernen schnell, Systeme zu manipulieren
:::

---

## Fazit zum sokratischen KI-Tutoring

**Sprechtext:**

Das Fazit: Die ehrliche Antwort ist: Wir wissen es noch nicht.

Links: Was plausibel ist. Selbsterklärung und Generierung fördern Lernen. Das ist gut etabliert.

Rechts: Was nicht belegt ist. Dass aktuelle KI-Systeme dies effektiv implementieren können.

[KLICK]

Ich denke, Sokrates würde es schätzen: Die beste Art, Werkzeuge zu bewerten, die seine Methode beanspruchen, ist, kritische Fragen zu stellen und unbegründete Antworten nicht zu akzeptieren.

::: {.callout-note}
## Hintergrund
Die sokratische Methode selbst erfordert Skepsis gegenüber unbegründeten Behauptungen. "Know thyself" und "I know that I know nothing" sind sokratische Prinzipien.

Die Anwendung auf KI-Tutoring: Bevor wir annehmen, dass KI sokratisch tutoren kann, sollten wir Evidenz fordern. Die Beweislast liegt bei denen, die Wirksamkeit behaupten.
:::

---

# Teil 5: Implikationen und Synthese

## Titelfolie: Teil 5

**Sprechtext:**

Jetzt kommen wir zur Synthese. Was bedeutet all das zusammen? Und was folgt daraus für die Praxis?

---

## Die unbequeme Wahrheit

**Sprechtext:**

Hier ist die unbequeme Wahrheit, die sich aus unserer Diskussion ergibt:

Was KI für Produktivität nützlich macht, macht sie für Lernen potenziell schädlich.

Sofortige Antworten eliminieren die Anstrengung, die Kompetenz aufbaut.

Dies ist kein Mangel aktueller KI. Es folgt aus etablierten Prinzipien der Kognitionswissenschaft. Wobei die direkte Evidenz für KI noch begrenzt ist, das muss ich hinzufügen.

::: {.callout-note}
## Hintergrund
Diese These ist die zentrale Aussage der Präsentation. Sie ist:
- **Theoretisch fundiert**: CLT, Desirable Difficulties, Generierungseffekt
- **Empirisch gestützt**: Bastani-Studie und verwandte Forschung
- **Aber noch nicht definitiv bewiesen**: Längsschnittstudien fehlen

Die epistemische Haltung sollte sein: Starke theoretische Gründe für Vorsicht, aber Offenheit für neue Evidenz.
:::

---

## Mind-extending vs. Mind-replacing

**Sprechtext:**

Der Philosoph Andy Clark bietet eine hilfreiche Unterscheidung.

Links: Kognition erweitern. Das Werkzeug erhält kognitives Engagement. Es leitet Anstrengung um auf höhere Aufgaben. Der Mensch bleibt in der Schleife. Fähigkeiten bleiben intakt.

Rechts: Kognition ersetzen. Das Werkzeug eliminiert Engagement. Es erzeugt Abhängigkeit. Der Mensch akzeptiert passiv. Fähigkeiten verkümmern.

[KLICK]

Dasselbe Werkzeug kann beides sein, abhängig von der Nutzung. Ein Experte, der Boilerplate-Code generieren lässt, erweitert seine Kognition. Ein Studierender, der die Hausaufgabe generieren lässt, ersetzt sie.

::: {.callout-note}
## Hintergrund zu Andy Clark
Andy Clark ist ein führender Philosoph des Geistes, bekannt für die "Extended Mind"-These. Sein aktueller Artikel (2025) wendet diese Ideen auf generative KI an.

Die Unterscheidung:
- **Mind-extending**: Werkzeug verstärkt menschliche Kognition
- **Mind-replacing**: Werkzeug übernimmt kognitive Aufgabe vollständig

Beispiele:
- Taschenrechner für komplexe Berechnungen: extending (ich verstehe was berechnet wird)
- Taschenrechner für einfache Arithmetik bei Kindern: potentially replacing (sie lernen nicht rechnen)

Übertragen auf KI: Die Frage ist nicht "KI nutzen oder nicht", sondern "wie nutzen".
:::

---

## Die Sequenzierungsfrage

**Sprechtext:**

Eine der wichtigsten praktischen Fragen ist die Sequenzierung.

[KLICK für jeden Punkt]

Studierende brauchen wahrscheinlich Grundwissen BEVOR KI vorteilhaft wird.

Der Expertise-Umkehr-Effekt legt dynamische Policies nahe, nicht pauschale Verbote oder Freigaben.

Ab welchem Punkt wechselt KI von schädlich zu hilfreich?

Die Antwort ist vermutlich domänen- und studentenspezifisch. Es gibt wahrscheinlich keine einfache Regel.

::: {.callout-note}
## Hintergrund zur Sequenzierung
Die Sequenzierungsfrage ist eine der wichtigsten praktischen Implikationen:

**Optionen**:
1. KI erst ab bestimmtem Kurs-Level erlauben
2. KI erst nach Grundlagen-Modul erlauben
3. KI mit abnehmender Unterstützung (Fading)
4. KI immer erlauben, aber Prüfungen ohne KI

**Herausforderung**: Die "richtige" Schwelle ist nicht bekannt und variiert wahrscheinlich nach:
- Fach/Domäne
- Aufgabentyp
- Individuellem Vorwissen

Dies spricht für experimentelle Ansätze und lokale Anpassung.
:::

---

## Die Entwicklungsfrage

**Sprechtext:**

Es gibt auch eine entwicklungspsychologische Dimension.

[KLICK für jeden Punkt]

Der präfrontale Kortex, zuständig für exekutive Funktionen, Metakognition und Selbstregulation, entwickelt sich bis Mitte 20.

Das bedeutet: Studierende sind noch in der Entwicklung dieser Fähigkeiten.

[KLICK]

Hier ist das Paradox: Jene, die KI-Nutzung am wenigsten regulieren können, sind am verletzlichsten für ihre negativen Effekte.

[KLICK]

Die Kohortenfrage: Die aktuelle Studierendengeneration ist möglicherweise die erste, die ihre gesamte Bildungslaufbahn mit generativer KI durchläuft.

Wenn wir Langzeitdaten haben, wird eine Generation bereits das Experiment gewesen sein.

::: {.callout-note}
## Hintergrund zur Entwicklungsperspektive
Die Hirnentwicklung bis Mitte 20 ist gut dokumentiert:
- **Präfrontaler Kortex**: Letzte Region, die ausreift
- **Funktionen**: Planung, Impulskontrolle, Metakognition

Implikation: Studierende haben noch nicht die volle Kapazität für Selbstregulation und kritische Bewertung.

Die "natürliche Experiment"-Perspektive ist beunruhigend: Wenn KI-Nutzung während der Bildung schädlich ist, werden wir das erst wissen, wenn es zu spät ist, für die betroffene Kohorte etwas zu ändern.

Dies spricht für das Vorsichtsprinzip: Bei Unsicherheit über langfristige Schäden, konservativ handeln.
:::

---

## Die soziale Dimension

**Sprechtext:**

Es gibt auch eine soziale Dimension, die Justin Reich betont.

[KLICK für jeden Punkt]

Lernen ist nicht nur ein kognitiver, sondern ein sozialer Prozess.

Studierende lernen besser, wenn sie sich verbunden fühlen. Peers werden seltener konsultiert, wenn KI antwortet. Beziehungen zu Lehrenden werden oberflächlicher, wenn KI die erste Anlaufstelle ist. Gelegenheiten für Mentoring nehmen ab.

[KLICK]

Die Rolle der Lehrperson geht über Informationsvermittlung hinaus: Beziehung, Motivation, Vorbild. Diese sind nicht ersetzbar.

::: {.callout-note}
## Hintergrund zur sozialen Dimension
Justin Reichs Forschung betont:
- **Relational Teaching**: Beziehung zwischen Lehrenden und Lernenden ist zentral
- **Social Learning**: Peer-Interaktion ist wichtig für Motivation und Verstehen
- **Hidden Curriculum**: Viel wird implizit durch soziale Interaktion gelernt

Wenn KI die erste Anlaufstelle bei Fragen wird:
- Peer-Diskussionen nehmen ab
- Office Hours werden weniger genutzt
- Die soziale Dimension des Lernens wird reduziert

Dies könnte Konsequenzen haben, die über rein kognitive Lerneffekte hinausgehen.
:::

---

## Die Equity-Dimension

**Sprechtext:**

Und dann ist da die Equity-Dimension.

[KLICK]

Die Idee der "digitalen Kluft" hat sich entwickelt. Die erste Kluft war der Zugang zu Geräten. Die zweite Kluft war die Fähigkeit zur sinnvollen Nutzung. Die dritte Kluft ist die Qualität der pädagogischen Integration.

[KLICK]

Das Risiko: Die "Demokratisierung" von Bildung durch KI könnte genau jene benachteiligen, denen sie helfen soll.

Gutausgestattete Studierende in ressourcenreichen Institutionen erhalten ausgeklügelte pädagogische Unterstützung mit KI.

Unterversorgte Studierende erhalten KI als unbeaufsichtigte Abkürzung.

::: {.callout-note}
## Hintergrund zur dritten digitalen Kluft
Der Begriff stammt von Trucano (2023) bei der Weltbank:

**Erste Kluft**: Wer hat Zugang zu Technologie?
**Zweite Kluft**: Wer kann Technologie sinnvoll nutzen?
**Dritte Kluft**: Wer profitiert pädagogisch von Technologie?

Die dritte Kluft ist die subtilste und potenziell folgenreichste:
- Priviligierte Studierende: KI als Erweiterung in gut gestalteten Lernumgebungen
- Benachteiligte Studierende: KI als Ersatz für fehlende Unterstützung

Dies ist das Gegenteil der versprochenen "Demokratisierung".
:::

---

## Der stärkste Gegeneinwand

**Sprechtext:**

Ich möchte den stärksten Gegeneinwand adressieren:

"Wenn KI immer verfügbar ist, müssen Fähigkeiten nicht internalisiert werden."

[KLICK für jede Antwort]

Erstens, die Permanenzannahme: Dieser Einwand setzt voraus, dass KI immer verfügbar, funktional und bezahlbar bleibt. Das ist eine starke Annahme über eine unbekannte Zukunft.

Zweitens, das Rekursionsproblem: Wer erkennt, wenn KI falsch liegt? Wer erweitert menschliches Wissen über das hinaus, was KI trainiert wurde?

Drittens, das Autonomie-Argument: Eigenständige kognitive Fähigkeit hat intrinsischen Wert, unabhängig von ihrer praktischen Notwendigkeit.

Viertens, unbekannte Unbekannte: Wir wissen nicht, welche Kaskadeneffekte folgen könnten, wenn grundlegende kognitive Fähigkeiten nicht entwickelt werden.

::: {.callout-note}
## Hintergrund zum Gegeneinwand
Der Einwand ist ernst zu nehmen. Die Antworten im Detail:

1. **Permanenzannahme**: Technologien können versagen, teuer werden, reguliert werden, oder sich verändern. Kompetenz ohne KI ist eine Versicherung.

2. **Rekursionsproblem**: KI ist auf menschlich generiertes Wissen trainiert. Wenn Menschen aufhören, Wissen zu entwickeln, was trainiert zukünftige KI?

3. **Autonomie**: Philosophisch: Ist ein Leben in Abhängigkeit von Werkzeugen ein gutes Leben? Praktisch: Selbstwirksamkeit korreliert mit Wohlbefinden.

4. **Unbekannte Unbekannte**: Die Kaskadeneffekte übersprungener kognitiver Entwicklung sind unerforscht.
:::

---

## Was wir noch nicht wissen

**Sprechtext:**

Ich möchte transparent sein darüber, was wir NICHT wissen.

[KLICK für jeden Punkt]

Längsschnittstudien über Jahre: Praktisch nicht vorhanden. Wir wissen nicht, was passiert, wenn Studierende jahrelang mit KI lernen.

Transfer auf neue Kontexte: Unerforscht. Können Studierende, die mit KI gelernt haben, in KI-freien Kontexten performen?

Optimale Scaffolding-Bedingungen: Unbekannt. Wann ist wie viel Unterstützung richtig?

Disziplinspezifische Effekte: Untererforscht. Gilt für MINT dasselbe wie für Geisteswissenschaften?

Publikationsbias: Wahrscheinlich vorhanden. Negative Ergebnisse werden weniger publiziert.

[KLICK]

Epistemische Bescheidenheit ist angebracht. Wir argumentieren von Theorie und begrenzter Evidenz.

::: {.callout-note}
## Hintergrund zur epistemischen Lage
Die Forschungslage ist dünn:
- Die meisten Studien sind kurzfristig (Wochen, nicht Jahre)
- Stichproben sind oft nicht repräsentativ
- Outcome-Masse variieren stark
- Replikationen fehlen

Dies bedeutet nicht, dass wir nichts wissen. Die theoretischen Grundlagen (CLT, Desirable Difficulties) sind solide. Aber die spezifische Anwendung auf KI ist noch hypothetisch.

Die richtige Haltung: Informierte Vorsicht, nicht Panik oder Ignoranz.
:::

---

## Abschliessende Provokationen

**Sprechtext:**

Ich schliesse mit einigen Provokationen.

[KLICK nach jedem Zitat]

"If AI assistance during education impairs independent capability, students may graduate less prepared for contexts where AI is unavailable."

"The productivity gains during education would come at the cost of capability thereafter."

"Whether this tradeoff is acceptable depends on assumptions about the future that educators cannot verify."

[KLICK]

Und schliesslich: "Wenn wir uns bei den Risiken irren, haben wir die Einführung nützlicher Technologie etwas verlangsamt. Wenn sich die Kritiker bei der Sicherheit irren, haben wir möglicherweise die kognitive Entwicklung einer Generation beeinträchtigt."

Die Asymmetrie der Risiken spricht für Vorsicht.

::: {.callout-note}
## Hintergrund zur Risiko-Asymmetrie
Das Argument folgt dem Vorsichtsprinzip:

**Wenn Skeptiker sich irren**: Wir haben KI-Adoption verlangsamt. Studierende haben etwas weniger Produktivitätsgewinne. Der Schaden ist begrenzt und reversibel.

**Wenn Optimisten sich irren**: Eine Generation hat kognitive Fähigkeiten nicht entwickelt. Der Schaden ist gross und möglicherweise irreversibel.

Bei asymmetrischen Risiken ist Vorsicht rational, auch ohne Gewissheit.
:::

---

## Offene Diskussion

**Sprechtext:**

Jetzt haben wir 15 Minuten für eine offene Diskussion.

Drei Fragen als Ausgangspunkt:

Was bedeutet das für deine Lehre? Welche konkreten Änderungen könntest du vornehmen?

Wo siehst du die Experten-Lernenden-Unterscheidung in der Praxis? Hast du Beispiele erlebt?

Was sind die schwierigsten Fragen, die das aufwirft? Worüber bist du unsicher?

[Timer läuft]

::: {.callout-note}
## Moderationshinweis
Diese abschliessende Diskussion soll die Teilnehmenden zur Reflexion und konkreten Planung anregen.

Mögliche Nachfragen:
- "Wer hat einen konkreten nächsten Schritt identifiziert?"
- "Wer sieht Widersprüche zwischen dem Gehörten und institutionellen Erwartungen?"
- "Welche Unterstützung würdet ihr brauchen, um informierte Entscheidungen zu treffen?"

Wichtig: Keine Einheitslösung propagieren. Die Teilnehmenden sollen befähigt werden, kontextspezifische Entscheidungen zu treffen.
:::

---

## Referenzen

**Sprechtext:**

Auf dieser Folie finden Sie die vollständigen Literaturangaben. Die Präsentation und die Referenzen werden online verfügbar sein.

Vielen Dank für Ihre Aufmerksamkeit und Ihre Beteiligung.

::: {.callout-note}
## Hintergrund
Zentrale Referenzen für Nachbereitung:
- Bastani et al. (2025): Die Hauptstudie zu KI und Lernen
- Sweller (2024): Aktueller Stand der Cognitive Load Theory
- Willingham (2008): Kritisches Denken und Domänenwissen
- Reich (2020): EdTech-Geschichte und Vorsicht
- Clark (2025): Mind-extending vs. mind-replacing
:::

---
title: "KI in der Hochschulbildung: Ein Leitfaden"
subtitle: "Werkzeuge für Experten, Herausforderungen für Lernende"
author:
  - name: Andrew Ellis
    url: https://github.com/awellis
    affiliation: Virtuelle Akademie, Berner Fachhochschule
    affiliation-url: https://virtuelleakademie.ch
    orcid: 0000-0002-2788-936X

date: 2025-12-10
toc: true
toc-depth: 3
number-sections: false
bibliography: ../bibliography.bib
---

## Einleitung: Das zentrale Paradox {#einleitung}

ChatGPT besteht Anwaltsprüfungen. Claude kann professionelle Programmierer erstezen. Gemini diagnostiziert Krankheiten. Als Hochschullehrende fragst du dich vermutlich: Wie integriere ich diese Werkzeuge sinnvoll in meine Lehre?

Die Antwort ist komplizierter, als viele annehmen. Eine Studie in *PNAS* [@bastaniGenerativeAIGuardrails2025] zeigt ein beunruhigendes Muster: Rund 1000 Gymnasiasten mit GPT-4-Zugang lösten 48% mehr Mathematikaufgaben korrekt. Als der KI-Zugang später entfernt wurde, schnitten dieselben Schüler jedoch 17% *schlechter* ab als jene, die nie KI hatten.

Das ist das *Produktivitäts-Lern-Paradox*: KI verbessert die Aufgabenleistung, kann aber das Lernen selbst beeinträchtigen. Die Frage ist nicht mehr *ob* KI hilft, sondern *wann* sie hilft und *wann* sie schadet.

Dieser Leitfaden bietet einen Rahmen, um diese Frage zu beantworten. Die zentrale These:

> KI-Werkzeuge sind primär für Experten konzipiert. Sie machen Experten produktiver, während Lernende oft nicht profitieren, weil Lernen die kognitive Anstrengung erfordert, die KI zu eliminieren droht. Ohne durchdachte Integration wird KI das Lernen eher beeinträchtigen als fördern.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Vor dem Weiterlesen

Bevor du diesen Leitfaden liest, reflektiere kurz:

- Wo hast du Studierende beim KI-Einsatz beobachtet?
- Welche Auswirkungen auf das Lernen hast du vermutet?
- Welche Fragen stellst du dir bezüglich KI in deiner Lehre?
Diese Reflexion hilft, die folgenden Konzepte auf den eigenen Kontext anzuwenden.
:::

## Was KI heute kann {#was-ki-kann}

### Grundlegende Funktionsweise

Bevor wir über Auswirkungen sprechen, lohnt sich ein Blick darauf, was diese Systeme eigentlich tun. Large Language Models (LLMs) wie GPT-5 oder Claude sind im Kern *Next-Word-Prediction-Systeme*[^1]. Sie wurden auf Milliarden von Textdokumenten trainiert und haben dabei Muster in Sprache, Argumentation und Stil gelernt, aber auch sehr viel Wissen. Die beste Metapher: extrem ausgeklügelte Autovervollständigung.

[^1]: Eigentlich "Next-Token-Prediction", da sie eigentlich Wortteile oder Satzzeichen vorhersagen.

Entscheidend ist die Unterscheidung zwischen *Retrieval* und *Generation*:

- *Retrieval* (wie eine Suchmaschine): Information wird gefunden und zurückgegeben
- *Generation* (wie ein LLM): Text wird neu erzeugt basierend auf statistischen Mustern

LLMs *rufen* kein Wissen ab. Sie *generieren* Text, der plausibel klingt. Dies erklärt, warum sie "halluzinieren"[^2] können: Sie optimieren für sprachliche Plausibilität, nicht für faktische Korrektheit.

[^2]: "Halluzinieren" ist eigentlich kein guter Begriff; ein treffenderer Ausdruck wäre "Konfabulation".

### Chain-of-Thought Reasoning

Moderne LLMs können "denken", indem sie ihre Überlegungen schrittweise externalisieren. Bei der Frage "Was ist 17 × 24?" kann ein LLM antworten:

> "Lass mich das aufteilen: 17 × 20 = 340, 17 × 4 = 68, 340 + 68 = 408"

Dieses *Chain-of-Thought Reasoning* verbessert die Leistung bei komplexen Aufgaben erheblich. Es ist jedoch immer noch Mustererkennung, nur über Denkschritte statt direkt über Antworten. Die KI hat gelernt, wie Menschen Probleme in Teilprobleme zerlegen.

### Werkzeugfähige Agenten

Die neueste Entwicklung sind *Agenten*: LLMs, die mit externen Werkzeugen verbunden sind. Sie können:

- Websuchen durchführen
- Code schreiben und ausführen
- Dateien lesen und bearbeiten
- Berechnungen anstellen
- Programmierschnittstellen (APIs) aufrufen

Die Konsequenz: Agenten können eine grosse Menge an kognitiven Aufgaben ausführen, von Literaturrecherche über Datenanalyse bis zum Schreiben und Überarbeiten von Texten. Die Fähigkeitsgrenze verschiebt sich ständig.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: KI-Fähigkeiten aktuell halten

Die Fähigkeiten von KI-Systemen entwickeln sich rasant. Empfehlungen:

1. Regelmässig aktualisieren: Überprüfe regelmässig, was aktuelle Modelle können
2. Selbst testen: Probiere neue Modelle mit Aufgaben aus deinem Fachgebiet
3. Skeptisch bleiben: Marketing-Behauptungen übertreffen oft die tatsächliche Leistung
4. Grenzen kennen: Identifiziere Aufgaben, bei denen KI zuverlässig versagt
:::

## Wie Expertise entsteht {#expertise}

Warum wirkt KI auf Experten und Lernende so unterschiedlich? Die Antwort ist darin zu finden, was Expertise eigentlich ist.

### Experten und Novizen sind grundlegend verschieden

Ein weit verbreitetes Missverständnis: Experten haben einfach "mehr Wissen". Die Forschung zeigt etwas anderes. Experten haben eine qualitativ andere *kognitive Architektur*.

Das klassische Beispiel stammt aus der Schachforschung [@grootThoughtChoiceChess1978; @chasePerceptionChess1973]. Schachmeistern und Anfängern wurden Stellungen für wenige Sekunden gezeigt. Bei echten Spielstellungen erinnerten Meister deutlich mehr Figuren korrekt als Anfänger. Bei zufällig platzierten Figuren waren beide Gruppen gleich schlecht.

Die Interpretation: Meister sehen nicht einzelne Figuren, sondern *Chunks*, bedeutungsvolle Muster wie "Königsangriff" oder "offene Linie". Diese Chunks sind im Langzeitgedächtnis gespeichert und werden automatisch erkannt.

Das Prinzip gilt domänenübergreifend:

- Ärzte sehen Symptomkomplexe, nicht Einzelsymptome
- Programmierer sehen Design Patterns, nicht Codezeilen
- Historiker sehen Epochenmerkmale, nicht Einzeldaten

### Von schwachen zu starken Methoden

Wie lösen Menschen Probleme? Newell und Simon [@newellHumanProblemSolving1972] unterschieden in ihrer einflussreichen Arbeit zwischen "schwachen" und "starken" Methoden der Problemlösung. Anderson's Theorie des Fertigkeitserwerbs [@andersonAcquisitionCognitiveSkill1982] beschreibt dann den Mechanismus, wie der Übergang zwischen diesen Methoden mit wachsender Expertise stattfindet.

Novizen nutzen schwache Methoden:

- Mittel-Ziel-Analyse: "Wo bin ich? Wo will ich hin? Was bringt mich näher?"
- Versuch und Irrtum
- Analogiebildung: "Das ist wie etwas, das ich schon kenne"
- Rückwärtsarbeiten vom Ziel

Diese Methoden heissen "schwach", weil sie domänenunabhängig und allgemein anwendbar, aber langsam, fehleranfällig und kognitiv anstrengend sind.

Experten nutzen starke Methoden:

- Automatische Mustererkennung: "Das ist ein Fall von X"
- Direkte Lösungswege: "Bei X macht man Y"
- Intuition basierend auf tausenden Erfahrungen

Der Übergang von schwachen zu starken Methoden erfordert umfangreiche Übung. Es gibt keine Abkürzung.

### Prozeduralisierung: Vom Wissen zum Können

Der Weg zur Expertise folgt typischen Phasen:

1. Deklaratives Wissen: Wissen als Fakten ("Man muss beim Autofahren die Kupplung treten, bevor man schaltet")
2. Bewusste Anwendung: Aktiv an jeden Schritt denken, langsam, fehleranfällig
3. Prozeduralisierung: Schritte werden zu Einheiten zusammengefasst ("Anfahren" statt drei separate Handlungen)
4. Automatisierung: Unbewusste, flüssige Ausführung

Automatisierte Prozesse belasten das Arbeitsgedächtnis nicht mehr. Sie sind schneller, zuverlässiger und setzen kognitive Ressourcen für höhere Aufgaben frei.

Entscheidend ist: Diese Transformation kann nicht übersprungen werden. Man kann nicht direkt von deklarativem Wissen zu Automatisierung springen. Der Weg führt durch bewusste, anstrengende Übung.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Übungsphasen schützen

Wenn KI die "langweiligen" Übungsphasen übernimmt, findet keine Prozeduralisierung statt. Empfehlungen:

1. Grundlagenphase KI-frei gestalten: Erste Begegnungen mit neuem Material ohne KI-Unterstützung
2. Übungszeit einplanen: Explizite Übungszeit, in der KI nicht erlaubt ist
3. Fortschritt monitoren: Regelmässig prüfen, ob Studierende Aufgaben auch ohne KI bewältigen
4. Sequenzierung beachten: Erst Grundlagen festigen, dann KI als Produktivitätswerkzeug einführen

Realitätscheck: Vollständige Kontrolle über KI-Nutzung ist kaum möglich. Fokus auf Prozessbewertung und Reflexion kann wirksamer sein als strikte Verbote.
:::

### Cognitive Load Theory: Das Nadelöhr des Lernens

Warum ist Übung so wichtig? Die Cognitive Load Theory [@swellerCognitiveLoadTheory2024] liefert die Antwort. Sie basiert auf zwei Gedächtnissystemen:

Arbeitsgedächtnis:

- Kapazität: etwa $4 \pm 1$ Elemente gleichzeitig (wobei dies von der Definition eines "Elements" und der Aufgabenart abhängt)
- Dauer: wenige Sekunden ohne aktive Aufrechterhaltung
- Der *Flaschenhals* allen Lernens

Langzeitgedächtnis:

- Praktisch unbegrenzte Kapazität
- Dauerhafte Speicherung
- Hier *lebt* Expertise

Die zentrale Erkenntnis: Alles Lernen muss durch den Flaschenhals des Arbeitsgedächtnisses. Wenn das Arbeitsgedächtnis überlastet ist, findet kein Lernen statt.

### Drei Arten kognitiver Belastung

Die Cognitive Load Theory unterscheidet drei Arten der Belastung:

| Typ | Beschreibung | Ziel |
|-----|--------------|------|
| **Intrinsisch** | Inhärente Komplexität des Materials | Kann nicht reduziert werden ohne Vereinfachung |
| **Extrinsisch** | Schlecht gestaltete Instruktion | Minimieren |
| **Lernförderlich** (Germane) | Produktive Anstrengung für Schemabildung | Erhalten |

(Anmerkung: Die Unterscheidung zwischen lernförderlicher und intrinsischer Belastung ist in der Literatur umstritten. Einige Forscher argumentieren, dass "germane load" keine eigenständige Kategorie darstellt, sondern die produktive Nutzung verfügbarer Kapazität beschreibt.)

Die entscheidende Frage für KI: Welche Art der Belastung reduziert sie?

- KI kann extrinsische Belastung reduzieren (z.B. bessere Erklärungen) → gut
- KI kann lernförderliche Belastung eliminieren (z.B. Antworten statt selbst denken) → problematisch

Das Ergebnis hängt vom Nutzungskontext ab.

### Der Expertise-Umkehr-Effekt

Ein Forschungsüberblick [@kalyugaExpertiseReversalEffect2009] zeigt einen bemerkenswerten Befund:

- Geringe Vorkenntnisse: Hohe Unterstützung hilft (mittlere bis grosse Effekte)
- Hohe Vorkenntnisse: Hohe Unterstützung schadet (die Effekte kehren sich um)

Das ist der *Expertise-Umkehr-Effekt*: Dieselbe Instruktionsmethode kann gegenteilige Effekte haben, abhängig vom Vorwissen der Lernenden.

Für Novizen reduziert Unterstützung die extrinsische Belastung und lässt Raum für Lernen. Für Experten ist die Unterstützung redundant und erzeugt zusätzliche Verarbeitungslast ("Ich weiss das schon, aber muss es trotzdem durcharbeiten").

Die Implikation für KI: Dasselbe KI-Werkzeug kann für Experten produktiv und für Novizen schädlich sein, oder umgekehrt, je nach Nutzungsweise. Es gibt keine "One-size-fits-all"-Lösung.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Unterstützung anpassen

Der Expertise-Umkehr-Effekt legt nahe, die KI-Nutzung an den Lernstand anzupassen:

1. Vorwissen erheben: Zu Beginn eines Kurses das Vorwissen einschätzen
2. Differenzieren: Unterschiedliche KI-Regeln für Anfänger und Fortgeschrittene
3. Fading einsetzen: Mit hoher Unterstützung beginnen, dann schrittweise reduzieren
4. Studierende einbeziehen: Über den Effekt informieren, Selbstregulation fördern
:::

### Was Instruktion leisten sollte: Explizite Instruktion

Bisher haben wir diskutiert, was KI für Lernen problematisch machen *kann*. Aber was *sollte* gute Instruktion tun? Die Forschung zu expliziter Instruktion [@kirschnerWhyMinimalGuidance2006] liefert klare Antworten.

Das Problem mit minimaler Anleitung: Konstruktivistische, entdeckende und problembasierte Ansätze klingen intuitiv attraktiv: Lernende sollen selbst entdecken, explorieren, Probleme lösen. Aber die empirische Evidenz zeigt konsistent: Für Novizen ist minimale Anleitung weniger effektiv als explizite Instruktion. Der Grund liegt in der kognitiven Architektur: Novizen haben keine Schemata im Langzeitgedächtnis, die sie zur Problemlösung nutzen könnten. Sie sind auf ihr begrenztes Arbeitsgedächtnis angewiesen, das schnell überlastet wird.

Worked Examples: Eine der am besten belegten Instruktionsmethoden für Novizen sind *Worked Examples* (ausgearbeitete Beispiele). Statt Lernende Probleme selbst lösen zu lassen, zeigt man ihnen vollständig ausgearbeitete Lösungswege. Das klingt kontraintuitiv: Sollten Lernende nicht selbst denken? Die Forschung zeigt: Für Novizen reduzieren Worked Examples die extrinsische kognitive Belastung und lassen Kapazität für Schemabildung. Der *Worked Example Effect* [@cooperEffectsSchemaAcquisition1987] ist einer der robustesten Befunde der Instruktionsforschung.

Der Unterschied zu KI: Hier liegt ein entscheidender Punkt: Worked Examples sind *nicht* dasselbe wie KI-generierte Lösungen.

- Worked Examples sind didaktisch gestaltet, heben relevante Schritte hervor, bauen systematisch Komplexität auf und werden von Lehrenden ausgewählt, um bestimmte Prinzipien zu illustrieren
- KI-generierte Antworten, in der typischen Nutzung durch Studierende, beantworten die gestellte Frage. Sie erfolgen jedoch ohne Einbettung in eine geplante Lernprogression. LLMs *können* mit entsprechendem Prompting didaktisch strukturierte Erklärungen liefern, aber das erfordert pädagogisches Wissen, das Novizen typischerweise nicht haben.

Der Unterschied liegt in der *Einbettung*: Worked Examples sind Teil eines durchdachten Instruktionsdesigns, das von Lehrenden mit Blick auf die Lernziele und den Wissensstand der Lernenden gestaltet wurde. KI-Antworten in der studentischen Alltagsnutzung fehlt dieser  Kontext.

Completion Problems: Ein Mittelweg sind *Completion Problems*: Teilweise ausgearbeitete Lösungen, die Lernende vervollständigen müssen. Sie bieten Struktur, fordern aber aktive kognitive Verarbeitung. Mit wachsender Expertise können die vorgefertigten Teile reduziert werden (*Fading*), bis Lernende Probleme vollständig selbst lösen.

Die Verbindung zum Expertise-Umkehr-Effekt: Explizite Instruktion und Worked Examples helfen Novizen, können aber Experten behindern (Expertise-Umkehr-Effekt). Daher ist *Fading* zentral: Unterstützung wird systematisch reduziert, wenn Expertise wächst. Das ist genau die dynamische Anpassung, die der Expertise-Umkehr-Effekt nahelegt.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Explizite Instruktion einsetzen

Evidenzbasierte Instruktion für Novizen:

1. Worked Examples nutzen: Ausgearbeitete Beispiele zeigen, bevor Lernende selbst lösen
2. Completion Problems einsetzen: Teilweise gelöste Aufgaben, die vervollständigt werden müssen
3. Fading planen: Mit viel Unterstützung beginnen, systematisch reduzieren
4. Didaktische Sequenzierung: Nicht jede Lösung zeigen, sondern gezielt ausgewählte Beispiele
5. KI nicht mit Worked Examples verwechseln: KI-Antworten ersetzen keine didaktisch gestaltete Instruktion

Der Kernpunkt: Novizen brauchen Unterstützung, aber die *richtige* Art von Unterstützung. Explizite Instruktion reduziert extrinsische Belastung, während produktive kognitive Arbeit erhalten bleibt. KI-Nutzung kann beides tun: extrinsische Belastung reduzieren (gut) oder die produktive Arbeit selbst übernehmen (problematisch).
:::

## Kritisches Denken erfordert Fachwissen {#kritisches-denken}

Studierende sollen lernen, KI kritisch zu nutzen. Das klingt vernünftig, greift aber zu kurz.

### Die traditionelle Annahme

Oft wird angenommen, kritisches Denken sei eine allgemeine, übertragbare Fähigkeit: einmal erworben, überall anwendbar.

Auf KI übertragen hiesse das: Man könnte Studierenden "KI-Kompetenz" beibringen, also die Fähigkeit, KI-Outputs kritisch zu bewerten, unabhängig vom Fachgebiet.

Aber stimmt diese Annahme?

### Willinghams Herausforderung

Daniel Willingham [@willinghamCriticalThinkingWhy2008] fasst die Forschung provokant zusammen:

> "Critical thinking is not a skill. There is not a set of critical thinking skills that can be acquired and deployed regardless of context."

Das ist zugespitzt formuliert, aber der Kern stimmt: Transfer ist schwieriger als oft angenommen.

### Evidenz für Domänenspezifität

Die Forschung zeigt wiederholt, dass Expertise nicht transferiert:

- Neurologen können Herzerkrankungen nicht gut diagnostizieren, obwohl sie medizinisch ausgebildet sind
- Fachredakteure können keine Zeitungsartikel schreiben, obwohl sie Texte redigieren können
- Selbst trainierte Philosophen werden von irrelevanten Merkmalen beeinflusst, wenn das Thema ausserhalb ihrer Expertise liegt

Willingham formuliert es so:

> "Abstract principles like 'look for hidden assumptions' won't help much in evaluating an argument about a topic you know little about."

### Angewendet auf KI-Bewertung

Was bedeutet das für die kritische Bewertung von KI-Outputs?

Eine Expertin in Biomedizin kann erkennen, wenn ChatGPT bei Biochemie falsch liegt. Sie hat die mentalen Modelle des Fachgebiets, kann "Das klingt falsch" erkennen, weiss welche Quellen zur Verifizierung dienen, und kann Plausibilität einschätzen.

Eine Novizin kann diese Bewertung nicht vornehmen, unabhängig von ihren "kritischen Denkfähigkeiten". Ihr fehlen die Referenzrahmen. Sie kann nicht unterscheiden zwischen plausibel und korrekt. Sie weiss nicht, welche Quellen autoritativ sind.

Was wie "kritisches Denken" aussieht, ist oft domänen-spezifisches Wissen.

### Was transferiert, und was nicht

Eine nuanciertere Betrachtung unterscheidet:

Transferiert teilweise:

- Planung des Vorgehens
- Überwachung des eigenen Verständnisses
- Selbstregulation
- Bereitschaft, Annahmen zu hinterfragen

Diese metakognitiven Strategien zeigen in der Forschung gewisse Generalisierung. Man kann sie domänenübergreifend lehren, und sie haben *etwas* Transferwirkung.

Transferiert kaum:

- Wissen, *was* in einem Fachgebiet plausibel ist
- Wissen, *welche* Quellen autoritativ sind
- Erkennen von *fachspezifischen* Fehlern

Diese inhaltliche Bewertungsfähigkeit erfordert Domänenwissen und transferiert kaum.

Der Kernpunkt: Die *Strategien* kann man lehren, aber ihre *Anwendung* erfordert Fachwissen. Die Strategie "Hinterfrage Annahmen" kann man lehren. Aber um zu wissen, *welche* Annahmen in einem biochemischen Text fragwürdig sind, braucht man Biochemie-Wissen.

### Die zentrale Implikation

Daraus folgen drei wichtige Erkenntnisse:

1. Studierende, die "mit hohem kritischem Denken" von KI profitieren, haben wahrscheinlich mehr Domänenexpertise. Studien, die zeigen, dass "kritische Denker" von KI profitieren, messen möglicherweise Vorwissen, nicht eine generische Fähigkeit.

2. Die beste Vorbereitung für kritische KI-Nutzung ist tiefes Fachlernen. Kontraintuitiv: Nicht "KI-Training", sondern Fachausbildung. Expertise ermöglicht kritische Nutzung; ohne Expertise ist Kritik kaum möglich.

3. Generische "KI-Kompetenz" kann Fachwissen ergänzen, aber nicht ersetzen. Workshops zu "Prompt Engineering" lösen das fundamentale Problem nicht. Die Fähigkeit, einen guten Prompt zu schreiben, ersetzt nicht die Fähigkeit, die Antwort zu bewerten.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Fach-spezifisches Lernen priorisieren

Anstatt nur "KI-Kompetenz" zu lehren:

1. Fachliche Grundlagen stärken: Mehr Zeit für Grundlagenwissen, nicht weniger
2. Domänenspezifische KI-Kritik üben: Im Fachkontext KI-Outputs gemeinsam analysieren
3. Fehler sammeln: Eine Sammlung typischer KI-Fehler im eigenen Fachgebiet anlegen
4. Metakognition fördern: Studierende lehren, ihr eigenes Verständnis zu überwachen
5. Realistische Erwartungen setzen: KI-Kritik erfordert Fachwissen, das Zeit braucht
:::

## Das Produktivitäts-Lern-Paradox {#paradox}

Die zentrale Frage lautet: Warum kann KI die Aufgabenleistung verbessern und gleichzeitig das Lernen beeinträchtigen?

### Die zentrale Unterscheidung

> "Learning and task completion are not synonymous." [@joseOutsourcingCognitionPsychological2025]

Diese Unterscheidung ist entscheidend:

- *Aufgabenleistung (Performance)*: Wie gut man eine Aufgabe *jetzt* löst
- *Lernen (Learning)*: Die Fähigkeit, ähnliche Aufgaben *später unabhängig* zu lösen

KI verbessert Aufgabenleistung (eindeutig belegt). Aber das sagt nichts über Lernen. Lernen erfordert möglicherweise genau die Anstrengung, die KI eliminiert.

### Die Bastani-Studie im Detail

Schauen wir uns die eingangs erwähnte Studie [@bastaniGenerativeAIGuardrails2025] genauer an:

Design:

- Rund 1000 türkische Gymnasiasten
- Randomisierte Zuweisung zu drei Gruppen
- Mathe-Übungen über mehrere Wochen
- Test am Ende ohne KI-Zugang

Die Bedingungen:

1. Kontrollgruppe: Kein KI-Zugang
2. Direkter GPT-4-Zugang: Freie Nutzung
3. GPT Tutor: Strukturierte Nutzung mit pädagogischen Leitplanken

Die Ergebnisse:

- Mit direktem GPT-4: 48% mehr Aufgaben gelöst
- Mit GPT Tutor: 127% mehr Aufgaben gelöst
- Ohne KI (später): 17% schlechter als Kontrollgruppe

Die Autoren fassen zusammen:

> "Students attempt to use GPT-4 as a 'crutch' during practice sessions, and when successful, perform worse on their own."

Wichtige Nuance: Die negativen Effekte betrafen primär den direkten Zugang. Der "GPT Tutor" zeigte bessere Ergebnisse, aber selbst mit Tutor war die spätere Leistung ohne KI reduziert. Die Art der KI-Nutzung macht einen Unterschied.

Einschränkungen: Die Studie hat methodische Limitationen: Der Kontext (türkische Gymnasiasten, Mathematik) ist spezifisch, die Effekte sind kurzfristig gemessen, und Replikationen stehen aus. Allerdings: Die Kernaussage, dass Aufgabenleistung und Lernen auseinanderfallen können, ist keine Überraschung. Sie folgt direkt aus der Cognitive Load Theory und den Prinzipien erwünschter Schwierigkeiten. Die Studie liefert empirische Evidenz für die Vorhersagen der Theorie.

### Desirable Difficulties: Warum Anstrengung nötig ist

Robert Bjork [@bjorkMakingThingsHard2011] hat das Konzept der "erwünschten Schwierigkeiten" geprägt:

> "Conditions that slow the rate of apparent learning often optimize long-term retention and transfer."

Vier bewährte Interventionen illustrieren das Prinzip:

1. Variation: Lernen unter wechselnden Bedingungen
2. Interleaving: Aufgabentypen mischen statt blocken
3. Spacing: Verteilt lernen statt massiert
4. Retrieval Practice: Aktiv abrufen statt passiv wiederlesen [@roedigerTestenhancedLearningTaking2006]

Alle vier haben gemeinsam: Sie fühlen sich *schwerer* an, sind aber *effektiver* für langfristiges Lernen.

Der Mechanismus: Sofortiger KI-Zugang kann Abrufversuche kurzschliessen. Statt selbst nachzudenken ("Was weiss ich darüber?"), fragt man die KI. Die Gedächtnisspur wird nicht gestärkt.

### Der Generierungseffekt

Der Generierungseffekt ist ein robuster Befund: Selbst generierte Information wird besser behalten als passiv erhaltene. Slamecka und Graf [-@slameckaGenerationEffectDelineation1978] demonstrierten dies erstmals experimentell, spätere Meta-Analysen bestätigten moderate Effektstärken.

Das typische Experiment:

- Gruppe A: Liest Wortpaare (Heiss---Kalt)
- Gruppe B: Ergänzt Wortpaare (Heiss---K___)
- Test: Beide Gruppen werden abgefragt
- Ergebnis: Gruppe B erinnert besser

Warum funktioniert es?

- Aktivere Verarbeitung während des Generierens
- Mehr Verbindungen im Gedächtnis
- Tiefere Enkodierung

Die Implikation: Wenn KI generiert, was Studierende selbst produzieren sollten, wird der Generierungseffekt eliminiert. Studierende, die selbst einen Essay-Entwurf schreiben, profitieren vom Generierungseffekt. Studierende, die einen KI-generierten Entwurf bearbeiten, nicht, auch wenn das Endprodukt ähnlich aussieht.

### Die Scaffolding-Hypothese

Eine tiefere Frage drängt sich auf: Was passiert mit der Entwicklung kognitiver Fähigkeiten? Hier müssen wir sorgfältig zwischen dem unterscheiden, was wir wissen, was wir theoretisch ableiten, und was wir vermuten.

Was wir wissen (Evidenz): Atrophie von Fähigkeiten ist gut dokumentiert. Eine vorhandene Fähigkeit verkümmert durch Nichtgebrauch. Das ist reversibel: Wenn man wieder übt, kommt die Fähigkeit zurück. Der Generierungseffekt und die Prinzipien erwünschter Schwierigkeiten sind empirisch belegt.

Was wir theoretisch ableiten: Wenn KI die kognitiven Prozesse übernimmt, die für Lernen notwendig sind, sollte weniger Lernen stattfinden. Das folgt aus der Cognitive Load Theory und ist durch Studien wie Bastani gestützt.

Was wir vermuten (Hypothese): Es könnte einen Unterschied geben zwischen Fertigkeitsatrophie und Entwicklungsbeeinträchtigung. Eine Fähigkeit, die nie entsteht, weil der konstruktive Prozess übersprungen wird, könnte schwerer nachzuholen sein als eine verkümmerte Fähigkeit. Die Hypothese: Grundfertigkeiten wie Schreiben, Rechnen und analytisches Lesen sind nicht nur *Fertigkeiten*, sondern *Prozesse*, die kognitive Architektur aufbauen. Schreiben könnte ein "epistemisches Werkzeug" sein: Gedanken entwickeln sich *durch* das Schreiben, nicht vor dem Schreiben.

Vorsicht: Diese Hypothese ist plausibel, aber nicht empirisch belegt. Wir haben keine Längsschnittstudien, die zeigen, dass übersprungene kognitive Entwicklungsphasen irreversible Defizite verursachen. Die Bedenken verdienen Aufmerksamkeit, sollten aber nicht als gesicherte Fakten behandelt werden.

### Historische Analogien

Frühere Technologien zeigen ähnliche Muster:

GPS und räumliches Gedächtnis [@dahmaniHabitualUseGPS2020]: Eine longitudinale Studie über drei Jahre zeigte: Stärkere GPS-Nutzung korreliert mit steilerem Rückgang des räumlichen Gedächtnisses. Die zeitliche Abfolge ist konsistent mit der Interpretation, dass GPS-Nutzung zum Rückgang beiträgt, wobei ungemessene konfundierende Variablen nicht ausgeschlossen werden können.

Konzeptuelles Verständnis [@lortie-forguesConceptualKnowledgeDecimal2017]: Forschung zeigt, dass selbst Erwachsene mit Zugang zu Rechenhilfen oft überraschende Lücken im konzeptuellen Verständnis mathematischer Operationen aufweisen. Werkzeugnutzung ersetzt kein Grundverständnis. Dieselbe Parallele wie bei KI: Das Werkzeug kann prozedurale Aufgaben übernehmen, aber konzeptuelles Verständnis muss eigenständig aufgebaut werden.

Google-Effekt [@sparrowGoogleEffectsMemory2011]: Menschen erinnern Information schlechter, wenn sie erwarten, dass sie verfügbar bleibt. Sie erinnern stattdessen, *wo* die Information zu finden ist. Das Gedächtnis adaptiert sich an die Verfügbarkeit externer Speicher.

### Grenzen der Analogien

Die historischen Analogien sind suggestiv, aber nicht ausreichend:

- GPS betrifft räumliche Navigation
- Taschenrechner betreffen arithmetische Berechnungen
- Google betrifft Informationsabruf

Jede dieser Technologien betrifft eine spezifische, enge kognitive Funktion.

Generative KI kann fast jede kognitive Aufgabe übernehmen: Schreiben, Argumentieren, Analysieren, Synthetisieren, Bewerten. Die Breite ist beispiellos. Wir können nicht einfach extrapolieren.

Aber: Die historischen Bedenken hatten oft Berechtigung. GPS *beeinflusst* tatsächlich räumliche Kognition. Taschenrechner *veränderten* den Mathematikunterricht. Die Bedenken waren nicht blosse Panikmache.

### Der EdTech-Hype-Zyklus

Bildungstechnologien folgen einem wiederkehrenden Muster [@reichFailureDisruptWhy2020]: Überschwängliche Versprechen, gefolgt von bescheidener Adoption, die bestehende Praktiken eher ergänzt als ersetzt.

- Radio sollte die besten Vorlesungen in jedes Klassenzimmer bringen
- Fernsehen sollte Lernen revolutionieren
- Computer sollten Unterricht personalisieren
- MOOCs sollten Elite-Bildung demokratisieren

Jede Technologie fand eine Nische, aber keine erfüllte die transformativen Versprechen.

Das MOOC-Beispiel ist besonders lehrreich: MOOCs versprachen Demokratisierung, erreichten aber primär Lernende, die bereits Abschlüsse hatten und berufliche Weiterbildung suchten. Der Erfolg erforderte genau die Selbstregulation und das Vorwissen, das privilegierte Lernende bereits besassen. Die "Demokratisierung" verstärkte bestehende Ungleichheiten.

Was macht dieses Mal anders? KIs Breite ist beispiellos. Aber dieselben strukturellen Kräfte könnten wirken: die Komplexität des Lehrens, die Einpassung neuer Werkzeuge in bestehende Praktiken, die Kluft zwischen Pilotprojekten und flächendeckender Umsetzung.

### "Das haben sie über das Schreiben auch gesagt"

Ein häufiger Einwand lautet: Sokrates warnte vor der Schrift, und wir haben überlebt. Jede neue Technologie löst Panik aus.

Im Phaidros warnte Sokrates: Schrift wird das Gedächtnis schwächen und nur "Scheinwissen" erzeugen.

Drei Antworten:

1. Schrift *hat* Kognition tiefgreifend verändert. Wir denken anders als orale Kulturen. Abstraktion, Kategorisierung, lineare Argumentation wurden durch Schrift gefördert. Das war nicht nur positiv oder negativ, es war transformativ.

2. Einige Bedenken waren berechtigt. Mündliche Gedächtnistraditionen *sind* zurückgegangen. Homers Epen wurden über Generationen mündlich überliefert. Diese Fähigkeit ist weitgehend verloren.

3. Schriftkultur entwickelte sich über Jahrhunderte. Es gab Zeit für kulturelle Anpassung. Bildungssysteme entwickelten sich mit. KI-Integration geschieht in Jahren, nicht Jahrhunderten.

### Warum Experten profitieren, Lernende nicht

Jetzt können wir versuchen, das Muster zu erklären:

Experten können:

- Routine-Aufgaben sicher auslagern (sie wissen, was "Routine" ist)
- Höheres Denken aufrechterhalten (kognitive Kapazität wird frei)
- KI-Outputs bewerten (sie haben Domänenexpertise)
- Ihre Grundfähigkeiten verkümmern nicht (sie sind schon da)

Lernenden fehlt:

- Wissen zur Bewertung (sie können nicht einschätzen, ob KI richtig liegt)
- Etablierte Grundfähigkeiten (was nicht da ist, kann nicht verkümmern, entsteht aber auch nicht)
- Metakognitive Kontrolle (sie wissen nicht, wann KI-Nutzung schadet)

Das Ergebnis kann "fliessende Inkompetenz" sein: Anspruchsvoll wirkende Outputs ohne zugrundeliegendes Verständnis. Das KI-generierte Produkt sieht kompetent aus, das Wissen fehlt.

Dasselbe Werkzeug, fundamental unterschiedliche Auswirkungen.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Lernsituationen gestalten

Um produktive Anstrengung zu erhalten:

1. "Ohne-KI"-Phasen einplanen: Explizite Zeiten, in denen KI nicht erlaubt ist
2. Prozess bewerten, nicht nur Produkt: Zwischenschritte einfordern und bewerten
3. Retrieval Practice integrieren: Regelmässige Abrufübungen ohne Hilfsmittel
4. Spacing und Interleaving nutzen: Verteiltes, gemischtes Üben
5. Generierung fordern: Eigene Entwürfe vor KI-Unterstützung verlangen
6. Reflexion einbauen: Studierende über ihren Lernprozess nachdenken lassen

Realitätscheck: Diese Empfehlungen erfordern Zeit und Planung. Nicht jede Lehrveranstaltung kann alles umsetzen. Beginne mit einem oder zwei Punkten, die zu deinem Kontext passen.
:::

## Exkurs: Sokratisches Fragen in KI-Tutoren {#sokrates}

Ein konkretes Beispiel für den Hype-Evidenz-Gap: Viele EdTech-Unternehmen bewerben ihre KI-Tutoren mit "sokratischer Methode".

::: {.pro-tip collapse="true"}
## Was "sokratisch" eigentlich bedeutet

Die ursprüngliche sokratische Methode, wie sie in Platons Dialogen beschrieben wird, war kein sanftes Hinführen zu richtigen Antworten. Sokrates stellte bohrende Fragen, die vermeintliches Wissen als unbegründet entlarvten. Das Ziel war *Aporie*: die Erkenntnis, dass man weniger weiss, als man dachte. Diese Erfahrung war oft unangenehm.

Was EdTech-Unternehmen "sokratisch" nennen, ist etwas anderes: ein System, das durch Fragen zur richtigen Antwort führt, statt sie direkt zu geben. Das ist eher *geleitetes Entdecken* als sokratischer Dialog. Die Bezeichnung klingt gut, aber der Vergleich hinkt.
:::

### Das Versprechen

Die Argumentation ist verlockend:

- Blooms "Two Sigma Problem" [@bloom2SigmaProblem1984]: 1:1-Tutoring erzielt 2 Standardabweichungen Verbesserung
- Das würde einen durchschnittlichen Studierenden in die Top 2% bringen
- KI kann unbegrenzt viele Lernende betreuen
- Also: Demokratisierung personalisierter Bildung

Aber: Die Begeisterung übersteigt die Evidenz erheblich.

### Warum Fragen theoretisch helfen könnten

Die kognitionswissenschaftliche Grundlage ist solide:

Generierungseffekt: Selbst erzeugte Antworten werden besser behalten als passiv erhaltene [@slameckaGenerationEffectDelineation1978].

Selbsterklärungseffekt: Erklären fördert tiefere Verarbeitung und deckt Wissenslücken auf [@chiElicitingSelfExplanationsImproves1994].

Aber: Die Qualität der Selbsterklärungen und damit der Lerneffekt hängt vom Vorwissen ab. Das verknüpft mit dem Expertise-Umkehr-Effekt: Was für Fortgeschrittene funktioniert, kann Novizen überfordern.

### Was die Evidenz tatsächlich zeigt

VanLehn [-@vanlehnRelativeEffectivenessHuman2011] führte eine Meta-Analyse zur Effektivität verschiedener Tutoring-Formen durch:

- Menschliche Tutoren: d = 0.79 (nicht 2.0 wie Bloom behauptete)
- Intelligente Tutorsysteme: d = 0.76 (vergleichbar)

VanLehns Analyse legt nahe, dass Schritt-für-Schritt-Feedback ein wesentlicher Faktor war, wobei die genaue Rolle des sokratischen Dialogs schwer zu isolieren ist.

Zu LLM-basierten sokratischen Tutoren: Es gibt keine gut kontrollierten randomisierten Studien. Die existierende Evidenz besteht hauptsächlich aus Zufriedenheitsumfragen und Vergleichen mit "kein Tutoring" statt mit Alternativen (siehe auch @weidlichChatGPTEducationEffect2025).

### Das Diagnose-Problem

Effektives sokratisches Fragen erfordert:

- Genaue Einschätzung des aktuellen Wissensstands
- Unterscheidung verschiedener Fehlertypen
- Anpassung der Fragen an den individuellen Lernenden

Beispiel: Wenn ein Schüler antwortet $\frac{1}{2} + \frac{1}{3} = \frac{2}{5}$, kann das bedeuten:

- Prozeduraler Fehler (Zähler und Nenner addiert)
- Konzeptueller Fehler (versteht nicht, was Brüche repräsentieren)
- Flüchtigkeitsfehler (weiss es eigentlich)

Die angemessene sokratische Frage unterscheidet sich je nach Ursache. Menschliche Tutoren nutzen Mimik, Tonfall, Zögern und jahrelange Erfahrung zur Diagnose. KI-Systeme haben nur den Text und können diese Unterscheidung nicht zuverlässig treffen.

### Weitere Herausforderungen

Fragesequenzierung: Sokratischer Dialog ist kontingent. Jede Frage hängt von der vorherigen Antwort ab. Einfache LLM-Implementierungen generieren Token für Token ohne expliziten pädagogischen Plan, obwohl Multi-Agent-Systeme oder strukturierte Prompting-Ansätze dies teilweise adressieren können.

Feedback-Timing: Wann korrigieren, wann weiter fragen lassen? Zu früh verhindert eigenes Denken, zu spät frustriert und verfestigt Fehler. Die Balance hängt vom individuellen Lernenden ab.

LLM-Sycophancy: LLMs sind darauf trainiert, hilfreich und angenehm zu sein. Sie tendieren dazu, Nutzern zuzustimmen. Das ist das Gegenteil von produktivem sokratischem Unbehagen. Sokrates machte seine Gesprächspartner unbequem. Das war der Punkt.

### Fazit zum sokratischen KI-Tutoring

Die ehrliche Antwort ist: Wir wissen es noch nicht.

*Was plausibel ist:* Selbsterklärung und Generierung fördern Lernen. Fragen können diese Prozesse anregen.

*Was nicht belegt ist:* Dass aktuelle KI-Systeme die nötige Diagnose leisten können. Dass LLM-basierte sokratische Tutoren besser sind als Alternativen. Dass positive Effekte langfristig halten.

Sokrates würde es schätzen: Die beste Art, Werkzeuge zu bewerten, die seine Methode beanspruchen, ist, kritische Fragen zu stellen und unbegründete Antworten nicht zu akzeptieren.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: KI-Tutoren evaluieren

Bei der Evaluation von KI-Tutoring-Tools:

1. Evidenz verlangen: Peer-Review-Studien mit Learning Outcomes, nicht nur Zufriedenheit
2. Diagnose-Fähigkeit prüfen: Kann das System zwischen Fehlertypen unterscheiden?
3. Opportunitätskosten bedenken: Ist KI-Tutoring besser als Lehrbuch, Übungsaufgaben, Peer-Diskussion?
4. Mit strukturierten Domänen beginnen: Mathematik vor Literaturanalyse
5. Auf unbeabsichtigte Folgen achten: Strategisches Ausnutzen des Systems statt echtem Lernen, Abhängigkeit von der KI-Unterstützung
6. Pilotieren und messen: Kleine Versuche mit klaren Erfolgskriterien
:::

## Implikationen und offene Fragen {#implikationen}

### Die unbequeme Wahrheit

Was bedeutet das alles? Die zentrale These lässt sich nun präzisieren:

> Was KI für Produktivität nützlich macht, droht sie für Lernen schädlich zu machen: Sofortige Antworten können die Anstrengung eliminieren, die Kompetenz aufbaut.

Dies ist kein Mangel aktueller KI. Es folgt aus etablierten Prinzipien der Kognitionswissenschaft. Das Problem liegt in der Natur des Lernens selbst. Die Frage ist daher nicht *ob*, sondern *wie* KI eingesetzt wird.

Einschränkung: Es gibt noch wenige Studien, die *direkt* messen, wie KI-Nutzung Lernen über längere Zeit beeinflusst. Die theoretische Argumentation basiert auf etablierten kognitionswissenschaftlichen Prinzipien, aber empirische Langzeitstudien zu generativer KI fehlen noch.

### Kognition erweitern vs. ersetzen

Andy Clark [@clarkExtendingMindsGenerative2025] bietet eine praktische Unterscheidung bei der Bewertung von KI-Nutzung:

Kognition erweitern:

- Der Mensch bleibt kognitiv engagiert
- Werkzeug verstärkt, ersetzt nicht
- Beispiel: Taschenrechner für einen Mathematiker
- Fähigkeiten bleiben erhalten und werden ausgebaut

Kognition ersetzen:

- Der Mensch wird passiv
- Werkzeug übernimmt das Denken
- Beispiel: KI schreibt den Essay, Studierender submittet
- Abhängigkeit entsteht, Fähigkeiten verkümmern

Dasselbe Werkzeug kann beides sein, abhängig von der Nutzung. Die Frage ist nicht "KI ja oder nein?", sondern "Wie wird KI genutzt?"

Aus Sicht der Cognitive Load Theory ist dabei entscheidend: Wissen, das im Langzeitgedächtnis gespeichert ist, unterscheidet sich fundamental von Wissen, auf das man extern zugreifen kann. Internalisiertes Wissen ermöglicht automatische Mustererkennung, befreit das Arbeitsgedächtnis und erlaubt höheres Denken. Externer Zugang erfordert immer bewusste Abrufprozesse und belastet das Arbeitsgedächtnis.

### Die Sequenzierungsfrage

Was bedeutet das praktisch? Die Forschung legt nahe:

- Studierende brauchen wahrscheinlich Grundwissen, bevor KI vorteilhaft wird
- Der Expertise-Umkehr-Effekt empfiehlt dynamische KI-Nutzungsregeln
- Die Schwelle, ab der KI von schädlich zu hilfreich wechselt, ist unbekannt
- Die Antwort ist vermutlich domänen- und personenspezifisch

Pauschale Empfehlungen zu geben ist schwierig. Die Forschung liefert Prinzipien, aber deren Anwendung erfordert kontextspezifisches Urteilsvermögen und ist abhängig vom Fach, vom Vorwissen und den Lernzielen.

### Die Entwicklungsfrage

Ein besonderes Anliegen betrifft die kognitive Entwicklung:

- Der präfrontale Kortex entwickelt sich bis etwa Mitte 20, wobei verschiedene Funktionen unterschiedlich schnell reifen und individuelle Variation erheblich ist
- Exekutive Funktionen, Metakognition, Selbstregulation sind bei vielen Studierenden noch in Entwicklung
- Jene, die KI-Nutzung am wenigsten regulieren können, sind möglicherweise am verletzlichsten

Die aktuelle Studierendengeneration könnte die erste sein, die ihre gesamte Bildungslaufbahn mit generativer KI durchläuft. Wenn wir in 20 Jahren Langzeitdaten haben, wird eine Generation bereits das Experiment gewesen sein.

### Die soziale Dimension

Dieser Leitfaden fokussiert auf kognitive Prozesse: Arbeitsgedächtnis, Schemabildung, Abrufübung. Aber Lernen ist nicht nur ein individueller kognitiver Prozess. Es ist fundamental sozial [@reichFailureDisruptWhy2020][^3].

Beziehungen beeinflussen Lernerfolg. Studierende lernen besser, wenn sie sich mit Lehrenden und Peers verbunden fühlen. Sie zeigen mehr Ausdauer, wenn sie spüren, dass ihre Lehrenden sich für ihren Erfolg interessieren. Disziplinäre Identität entsteht durch Gemeinschaften.

Was passiert, wenn Studierende KI fragen statt Menschen?

- Peers werden seltener konsultiert; kollaboratives Lernen leidet
- Beziehungen zu Lehrenden werden oberflächlicher, wenn Rückfragen an die KI gehen
- Gelegenheiten für Mentoring und informelles Lernen nehmen ab

Die Rolle der Lehrperson: Wenn KI sofortige Antworten und Feedback liefert, verschiebt sich die Rolle der Lehrenden. Die Frage ist, wie Lehrende ihre unersetzliche Funktion, nämlich Beziehung, Motivation, Vorbild, Kontext, neu definieren.

[^3]: Diese Dimension verdient mehr Aufmerksamkeit, als dieser Leitfaden ihr geben kann. Der Fokus auf Kognition ist eine bewusste Einschränkung.

### Die Equity-Dimension

Die "dritte digitale Kluft" [@michaeltrucanoAINextDigital2023] beschreibt ein neues Ungleichheitsmuster:

| Erste Kluft | Zweite Kluft | Dritte Kluft |
|-------------|--------------|--------------|
| Zugang zu Geräten | Fähigkeit zur sinnvollen Nutzung | Qualität der pädagogischen Integration |

Die Ironie: "Demokratisierung" durch KI könnte Ungleichheit verstärken:

- Gutausgestattete Studierende: ausgeklügelte pädagogische Unterstützung, informierte Betreuung, strukturierte KI-Nutzung
- Unterversorgte Studierende: KI als unbeaufsichtigte Abkürzung, niemand erklärt die Risiken

Konkrete Ungleichheiten:

- Kosten: Premium-KI-Werkzeuge kosten Geld. Wer kann sich ChatGPT Plus, Claude Pro oder spezialisierte Tools leisten? Wer ist auf kostenlose, limitierte Versionen angewiesen?
- Institutionelle Ressourcen: Welche Hochschulen haben Zeit und Expertise für durchdachte KI-Integration? Welche setzen KI ein, ohne Lehrende zu schulen?
- Betreuung: Wer hat Dozierende, die über KI-Risiken aufklären? Wer hat niemanden, der die Fragen stellt?

Das MOOC-Muster wiederholt sich möglicherweise: Eine Technologie, die "allen" zugänglich ist, nützt vor allem jenen, die bereits die Voraussetzungen mitbringen, sie produktiv zu nutzen.

### Der stärkste Gegeneinwand

Der stärkste Einwand lautet: "Wenn KI immer verfügbar ist, müssen Fähigkeiten nicht internalisiert werden."

Vier Antworten:

1. Permanenzannahme: Setzt voraus, dass KI immer verfügbar, funktional und bezahlbar bleibt. Stromausfall, Serverprobleme, Kosten, politische Entscheidungen können das ändern.

2. Rekursionsproblem: Wer erkennt, wenn KI falsch liegt? Wer trainiert die nächste KI-Generation? Wer erweitert menschliches Wissen? Irgendwer muss Domänenexpertise haben.

3. Autonomie-Argument: Eigenständiges Denken hat intrinsischen Wert für Selbstbestimmung, Würde, das Gefühl des Verstehens. Nicht alles lässt sich in Produktivität messen.

4. Unbekannte Unbekannte: Komplexe Systeme haben Kaskadeneffekte. Wir wissen nicht, was wir verlieren könnten.

### Was wir noch nicht wissen

Epistemische Bescheidenheit ist angebracht. Wir wissen vieles nicht:

- Längsschnittstudien über Jahre: Praktisch nicht vorhanden
- Transfer auf neue Kontexte: Unerforscht
- Optimale Scaffolding-Bedingungen: Unbekannt
- Disziplinspezifische Effekte: Untererforscht
- Publikationsbias: Wahrscheinlich vorhanden

### Abschliessende Überlegungen

Drei Beobachtungen fassen die Lage zusammen:

1. Wenn KI-Unterstützung während der Ausbildung die eigenständige Fähigkeit beeinträchtigt, könnten Studierende weniger vorbereitet sein auf Kontexte, in denen KI nicht verfügbar ist.

2. Die Produktivitätsgewinne während der Ausbildung könnten auf Kosten der späteren Kompetenz gehen.

3. Ob dieser Kompromiss akzeptabel ist, hängt von Annahmen über die Zukunft ab, die Lehrende nicht verifizieren können.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Entscheidungsrahmen

Für Entscheidungen über KI in der eigenen Lehre:

Fragen zur Selbstreflexion:

1. Welche kognitiven Prozesse will ich fördern?
2. Welche davon könnte KI übernehmen?
3. Ist die Übernahme für Lernen förderlich oder hinderlich?
4. Haben meine Studierenden das nötige Vorwissen für kritische KI-Nutzung?
5. Wie kann ich Grundlagen schützen und dennoch KI sinnvoll einsetzen?

:::

## Fazit {#fazit}

Die zentrale Botschaft: KI-Werkzeuge sind primär für Experten konzipiert. Sie machen Experten produktiver, während Lernende ohne durchdachte Integration oft nicht profitieren, weil Lernen die kognitive Anstrengung erfordert, die KI zu eliminieren droht.

Die Argumentation stützt sich auf:

- Cognitive Load Theory: Lernen erfordert produktive Anstrengung durch das Nadelöhr des Arbeitsgedächtnisses
- Expertise-Umkehr-Effekt: Dieselbe Unterstützung kann Novizen helfen und Experten schaden
- Domänenspezifität: Kritische KI-Bewertung erfordert Fachwissen, nicht nur generische Strategien
- Desirable Difficulties: Schwierigkeiten, die das Lernen verlangsamen, optimieren oft Langzeitbehalten
- Generierungseffekt: Selbst erzeugte Information wird besser behalten

Die praktischen Implikationen sind:

- Grundlagen vor Werkzeugen sequenzieren
- Prozess bewerten, nicht nur Produkt
- Nach Vorwissen differenzieren
- "Ohne-KI"-Phasen einplanen
- Kritische KI-Nutzung *im Fachkontext* üben

Die offenen Fragen sind zahlreich. Langzeitstudien fehlen, optimale Strategien sind unbekannt, und die Effekte variieren nach Kontext und Person.

Die Frage ist nicht, ob KI in die Bildung kommt. Sie ist schon da. Die Frage ist, wie wir sie so gestalten, dass sie dem Lernen dient, nicht es ersetzt.

## Referenzen {.unnumbered}

::: {#refs}
:::

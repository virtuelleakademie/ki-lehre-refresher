---
title: "KI in der Hochschulbildung: Ein Leitfaden"
subtitle: "Werkzeuge für Experten, Herausforderungen für Lernende"
author: "Dr. Andrew Ellis"
date: 2025-12-10
toc: true
toc-depth: 3
number-sections: true
bibliography: ../bibliography.bib
sidebar: guide
---

## Einleitung: Das zentrale Paradox {#einleitung}

ChatGPT besteht Anwaltsprüfungen. Claude schreibt funktionierenden Code. Gemini diagnostiziert Krankheiten. Als Hochschullehrende fragst du dich vermutlich: Wie integriere ich diese Werkzeuge sinnvoll in meine Lehre?

Die Antwort ist komplizierter, als viele annehmen. Eine aktuelle Studie [@bastaniGenerativeAICan2024] zeigt ein beunruhigendes Muster: Rund 1000 Gymnasiasten mit GPT-4-Zugang lösten 48% mehr Mathematikaufgaben korrekt. Als der KI-Zugang später entfernt wurde, schnitten dieselben Schüler jedoch 17% *schlechter* ab als jene, die nie KI hatten.

Das ist das **Produktivitäts-Lern-Paradox**: KI verbessert die Aufgabenleistung, kann aber das Lernen selbst beeinträchtigen. Die Frage ist nicht mehr *ob* KI hilft, sondern *wann* sie hilft und *wann* sie schadet.

Dieser Leitfaden bietet einen Rahmen, um diese Frage zu beantworten. Die zentrale These:

> **KI-Werkzeuge sind für Experten konzipiert. Sie machen Experten produktiver, während Lernende oft nicht profitieren, weil Lernen die kognitive Anstrengung erfordert, die KI eliminiert.**

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Vor dem Weiterlesen

Bevor du diesen Leitfaden liest, reflektiere kurz:

- Wo hast du Studierende beim KI-Einsatz beobachtet?
- Welche Auswirkungen auf das Lernen hast du vermutet?
- Welche Fragen stellst du dir bezüglich KI in deiner Lehre?

Diese Reflexion hilft, die folgenden Konzepte auf den eigenen Kontext anzuwenden.
:::

## Was KI heute kann {#was-ki-kann}

### Grundlegende Funktionsweise

Bevor wir über Auswirkungen sprechen, lohnt sich ein Blick darauf, was diese Systeme eigentlich tun. Large Language Models (LLMs) wie GPT-4 oder Claude sind im Kern **Next-Word-Prediction-Systeme**. Sie wurden auf Milliarden von Textdokumenten trainiert und haben dabei Muster in Sprache, Argumentation und Stil gelernt. Die beste Metapher: extrem ausgeklügelte Autovervollständigung.

Entscheidend ist die Unterscheidung zwischen *Retrieval* und *Generation*:

- **Retrieval** (wie eine Suchmaschine): Information wird gefunden und zurückgegeben
- **Generation** (wie ein LLM): Text wird neu erzeugt basierend auf statistischen Mustern

LLMs *rufen* kein Wissen ab. Sie *generieren* Text, der plausibel klingt. Dies erklärt, warum sie "halluzinieren" können: Sie optimieren für sprachliche Plausibilität, nicht für faktische Korrektheit.

### Chain-of-Thought Reasoning

Moderne LLMs können "denken", indem sie ihre Überlegungen schrittweise externalisieren. Bei der Frage "Was ist 17 × 24?" kann ein LLM antworten:

> "Lass mich das aufteilen: 17 × 20 = 340, 17 × 4 = 68, 340 + 68 = 408"

Dieses **Chain-of-Thought Reasoning** verbessert die Leistung bei komplexen Aufgaben erheblich. Es ist jedoch immer noch Mustererkennung, nur über Denkschritte statt direkt über Antworten. Die KI hat gelernt, wie Menschen Probleme zerlegen.

### Werkzeugfähige Agenten

Die neueste Entwicklung sind **Agenten**: LLMs, die mit externen Werkzeugen verbunden sind. Sie können:

- Websuchen durchführen
- Code schreiben und ausführen
- Dateien lesen und bearbeiten
- Berechnungen anstellen
- APIs aufrufen

Die Konsequenz: Agenten können fast jede kognitive Aufgabe ausführen, von Literaturrecherche über Datenanalyse bis zum Schreiben und Überarbeiten von Texten. Die Fähigkeitsgrenze verschiebt sich ständig.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: KI-Fähigkeiten aktuell halten

Die Fähigkeiten von KI-Systemen entwickeln sich rasant. Empfehlungen:

1. **Quartalsweise aktualisieren**: Überprüfe alle drei Monate, was aktuelle Modelle können
2. **Selbst testen**: Probiere neue Modelle mit Aufgaben aus deinem Fachgebiet
3. **Skeptisch bleiben**: Marketing-Behauptungen übertreffen oft die tatsächliche Leistung
4. **Grenzen kennen**: Identifiziere Aufgaben, bei denen KI zuverlässig versagt
:::

## Wie Expertise entsteht {#expertise}

Warum wirkt KI auf Experten und Lernende so unterschiedlich? Die Antwort liegt in dem, was Expertise eigentlich ist.

### Experten und Novizen sind grundlegend verschieden

Ein weit verbreitetes Missverständnis: Experten haben einfach "mehr Wissen". Die Forschung zeigt etwas anderes. Experten haben eine qualitativ andere **kognitive Architektur**.

Das klassische Beispiel stammt aus der Schachforschung [@grootThoughtChoiceChess1978; @chasePerceptionChess1973]. Schachmeistern und Anfängern wurden Stellungen für wenige Sekunden gezeigt. Bei echten Spielstellungen erinnerten Meister deutlich mehr Figuren korrekt als Anfänger. Bei zufällig platzierten Figuren waren beide Gruppen gleich schlecht.

Die Interpretation: Meister sehen nicht einzelne Figuren, sondern **Chunks**, bedeutungsvolle Muster wie "Königsangriff" oder "offene Linie". Diese Chunks sind im Langzeitgedächtnis gespeichert und werden automatisch erkannt.

Das Prinzip gilt domänenübergreifend:

- Ärzte sehen Symptomkomplexe, nicht Einzelsymptome
- Programmierer sehen Design Patterns, nicht Codezeilen
- Historiker sehen Epochenmerkmale, nicht Einzeldaten

### Von schwachen zu starken Methoden

Wie lösen Menschen Probleme? Andersons Theorie des Fertigkeitserwerbs [@andersonAcquisitionCognitiveSkill1982] beschreibt einen fundamentalen Wandel mit wachsender Expertise.

**Novizen nutzen schwache Methoden:**

- Mittel-Ziel-Analyse: "Wo bin ich? Wo will ich hin? Was bringt mich näher?"
- Versuch und Irrtum
- Analogiebildung: "Das ist wie etwas, das ich schon kenne"
- Rückwärtsarbeiten vom Ziel

Diese Methoden heissen "schwach", weil sie langsam, fehleranfällig und kognitiv anstrengend sind. Sie funktionieren aber in jeder Domäne.

**Experten nutzen starke Methoden:**

- Automatische Mustererkennung: "Das ist ein Fall von X"
- Direkte Lösungswege: "Bei X macht man Y"
- Intuition basierend auf tausenden Erfahrungen

Der Übergang von schwachen zu starken Methoden erfordert umfangreiche Übung. Es gibt keine Abkürzung.

### Prozeduralisierung: Vom Wissen zum Können

Der Weg zur Expertise folgt typischen Phasen:

1. **Deklaratives Wissen**: Wissen als Fakten ("Man muss beim Autofahren die Kupplung treten, bevor man schaltet")
2. **Bewusste Anwendung**: Aktiv an jeden Schritt denken, langsam, fehleranfällig
3. **Prozeduralisierung**: Schritte werden zu Einheiten zusammengefasst ("Anfahren" statt drei separate Handlungen)
4. **Automatisierung**: Unbewusste, flüssige Ausführung

Automatisierte Prozesse belasten das Arbeitsgedächtnis nicht mehr. Sie sind schneller, zuverlässiger und setzen kognitive Ressourcen für höhere Aufgaben frei.

**Entscheidend: Diese Transformation kann nicht übersprungen werden.** Man kann nicht direkt von deklarativem Wissen zu Automatisierung springen. Der Weg führt durch bewusste, anstrengende Übung.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Übungsphasen schützen

Wenn KI die "langweiligen" Übungsphasen übernimmt, findet keine Prozeduralisierung statt. Empfehlungen:

1. **Grundlagenphase KI-frei gestalten**: Erste Begegnungen mit neuem Material ohne KI-Unterstützung
2. **Übungszeit einplanen**: Explizite Übungszeit, in der KI nicht erlaubt ist
3. **Fortschritt monitoren**: Regelmässig prüfen, ob Studierende Aufgaben auch ohne KI bewältigen
4. **Sequenzierung beachten**: Erst Grundlagen festigen, dann KI als Produktivitätswerkzeug einführen
:::

### Cognitive Load Theory: Das Nadelöhr des Lernens

Warum ist Übung so wichtig? Die Cognitive Load Theory [@swellerCognitiveLoadTheory2024] liefert die Antwort. Sie basiert auf zwei Gedächtnissystemen:

**Arbeitsgedächtnis:**

- Kapazität: etwa 4±1 Elemente gleichzeitig (wobei dies von der Definition eines "Elements" und der Aufgabenart abhängt)
- Dauer: wenige Sekunden ohne aktive Aufrechterhaltung
- Der *Engpass* allen Lernens

**Langzeitgedächtnis:**

- Praktisch unbegrenzte Kapazität
- Dauerhafte Speicherung
- Hier *lebt* Expertise

Die zentrale Erkenntnis: **Alles Lernen muss durch das Nadelöhr des Arbeitsgedächtnisses.** Wenn das Arbeitsgedächtnis überlastet ist, findet kein Lernen statt.

### Drei Arten kognitiver Belastung

Die Cognitive Load Theory unterscheidet drei Arten der Belastung:

| Typ | Beschreibung | Ziel |
|-----|--------------|------|
| **Intrinsisch** | Inhärente Komplexität des Materials | Kann nicht reduziert werden ohne Vereinfachung |
| **Extrinsisch** | Schlecht gestaltete Instruktion | Minimieren |
| **Lernförderlich** (Germane) | Produktive Anstrengung für Schemabildung | Erhalten |

(Anmerkung: Die Unterscheidung zwischen lernförderlicher und intrinsischer Belastung ist in der Literatur umstritten. Einige Forscher argumentieren, dass "germane load" keine eigenständige Kategorie darstellt, sondern die produktive Nutzung verfügbarer Kapazität beschreibt.)

Die entscheidende Frage für KI: **Welche Art der Belastung reduziert sie?**

- KI kann extrinsische Belastung reduzieren (z.B. bessere Erklärungen) → gut
- KI kann lernförderliche Belastung eliminieren (z.B. Antworten statt selbst denken) → problematisch

Das Ergebnis hängt vom Nutzungskontext ab.

### Der Expertise-Umkehr-Effekt

Eine Meta-Analyse [@kalyugaExpertiseReversalEffect2009] zeigt einen bemerkenswerten Befund:

- **Geringe Vorkenntnisse**: Hohe Unterstützung hilft (d = 0.505)
- **Hohe Vorkenntnisse**: Hohe Unterstützung schadet (d = −0.428)

Das ist der **Expertise-Umkehr-Effekt**: Dieselbe Instruktionsmethode kann gegenteilige Effekte haben, abhängig vom Vorwissen der Lernenden.

Für Novizen reduziert Unterstützung die extrinsische Belastung und lässt Raum für Lernen. Für Experten ist die Unterstützung redundant und erzeugt zusätzliche Verarbeitungslast ("Ich weiss das schon, aber muss es trotzdem durcharbeiten").

**Die Implikation für KI**: Dasselbe KI-Werkzeug kann für Experten produktiv und für Novizen schädlich sein, oder umgekehrt, je nach Nutzungsweise. Es gibt keine "One-size-fits-all"-Lösung.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Unterstützung anpassen

Der Expertise-Umkehr-Effekt legt dynamische KI-Policies nahe:

1. **Vorwissen erheben**: Zu Beginn eines Kurses das Vorwissen einschätzen
2. **Differenzieren**: Unterschiedliche KI-Regeln für Anfänger und Fortgeschrittene
3. **Fading einsetzen**: Mit hoher Unterstützung beginnen, dann schrittweise reduzieren
4. **Studierende einbeziehen**: Über den Effekt informieren, Selbstregulation fördern
:::

## Kritisches Denken erfordert Fachwissen {#kritisches-denken}

"Wir müssen Studierenden beibringen, KI kritisch zu nutzen." Das klingt vernünftig. Es greift aber zu kurz.

### Die traditionelle Annahme

Die "21st Century Skills"-Bewegung hat kritisches Denken als übertragbare Kernkompetenz propagiert. Die Annahme: Es gibt eine allgemeine Fähigkeit "kritisches Denken", die in einem Kontext erworben und in anderen angewendet werden kann.

Auf KI übertragen: Man könnte Studierenden "KI-Kompetenz" beibringen, also die Fähigkeit, KI-Outputs kritisch zu bewerten, unabhängig vom Fachgebiet.

Aber stimmt diese Annahme?

### Willinghams Herausforderung

Daniel Willingham [@willinghamCriticalThinkingWhy2008] fasst die Forschung provokant zusammen:

> "Critical thinking is not a skill. There is not a set of critical thinking skills that can be acquired and deployed regardless of context."

Das ist zugespitzt formuliert, aber der Kern stimmt: **Transfer ist schwieriger als oft angenommen.**

### Evidenz für Domänenspezifität

Die Forschung zeigt wiederholt, dass Expertise nicht transferiert:

- Neurologen können Herzerkrankungen nicht gut diagnostizieren, obwohl sie medizinisch ausgebildet sind
- Fachredakteure können keine Zeitungsartikel schreiben, obwohl sie Texte redigieren können
- Selbst trainierte Philosophen werden von irrelevanten Merkmalen beeinflusst, wenn das Thema ausserhalb ihrer Expertise liegt

Willingham formuliert es so:

> "Abstract principles like 'look for hidden assumptions' won't help much in evaluating an argument about a topic you know little about."

### Angewendet auf KI-Bewertung

Was bedeutet das für die kritische Bewertung von KI-Outputs?

**Ein Experte in Biomedizin** kann erkennen, wenn ChatGPT bei Biochemie falsch liegt. Er hat die mentalen Modelle des Fachgebiets, kann "Das klingt falsch" erkennen, weiss welche Quellen zur Verifizierung dienen, und kann Plausibilität einschätzen.

**Ein Novize** kann diese Bewertung nicht vornehmen, unabhängig von seinen "kritischen Denkfähigkeiten". Ihm fehlen die Referenzrahmen. Er kann nicht unterscheiden zwischen plausibel und korrekt. Er weiss nicht, welche Quellen autoritativ sind.

**Was wie "kritisches Denken" aussieht, ist oft Domänenwissen in Aktion.**

### Was transferiert, und was nicht

Eine nuanciertere Betrachtung unterscheidet:

**Transferiert teilweise:**

- Planung des Vorgehens
- Überwachung des eigenen Verständnisses
- Selbstregulation
- Bereitschaft, Annahmen zu hinterfragen

Diese metakognitiven Strategien zeigen in der Forschung gewisse Generalisierung. Man kann sie domänenübergreifend lehren, und sie haben *etwas* Transferwirkung.

**Transferiert kaum:**

- Wissen, *was* in einem Fachgebiet plausibel ist
- Wissen, *welche* Quellen autoritativ sind
- Erkennen von *fachspezifischen* Fehlern

Diese inhaltliche Bewertungsfähigkeit erfordert Domänenwissen und transferiert kaum.

**Der Kernpunkt:** Die *Strategien* kann man lehren, aber ihre *Anwendung* erfordert Fachwissen. Die Strategie "Hinterfrage Annahmen" kann man lehren. Aber um zu wissen, *welche* Annahmen in einem biochemischen Text fragwürdig sind, braucht man Biochemie-Wissen.

### Die zentrale Implikation

Daraus folgen drei wichtige Erkenntnisse:

1. **Studierende, die "mit hohem kritischem Denken" von KI profitieren, haben wahrscheinlich mehr Domänenexpertise.** Studien, die zeigen, dass "kritische Denker" von KI profitieren, messen möglicherweise Vorwissen, nicht eine generische Fähigkeit.

2. **Die beste Vorbereitung für kritische KI-Nutzung ist tiefes Fachlernen.** Kontraintuitiv: Nicht "KI-Training", sondern Fachausbildung. Expertise ermöglicht kritische Nutzung; ohne Expertise keine Kritik.

3. **Generische "KI-Kompetenz" kann Fachwissen ergänzen, aber nicht ersetzen.** Workshops zu "Prompt Engineering" lösen das fundamentale Problem nicht. Die Fähigkeit, einen guten Prompt zu schreiben, ersetzt nicht die Fähigkeit, die Antwort zu bewerten.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Fachlernen priorisieren

Anstatt nur "KI-Kompetenz" zu lehren:

1. **Fachliche Grundlagen stärken**: Mehr Zeit für Grundlagenwissen, nicht weniger
2. **Domänenspezifische KI-Kritik üben**: Im Fachkontext KI-Outputs gemeinsam analysieren
3. **Fehler sammeln**: Eine Sammlung typischer KI-Fehler im eigenen Fachgebiet anlegen
4. **Metakognition fördern**: Studierende lehren, ihr eigenes Verständnis zu überwachen
5. **Realistische Erwartungen setzen**: KI-Kritik erfordert Fachwissen, das Zeit braucht
:::

## Das Produktivitäts-Lern-Paradox {#paradox}

Die zentrale Frage lautet: Warum kann KI die Aufgabenleistung verbessern und gleichzeitig das Lernen beeinträchtigen?

### Die zentrale Unterscheidung

> "Learning and task completion are not synonymous." [@joseCognitiveParadoxAI2025]

Diese Unterscheidung ist entscheidend:

- **Aufgabenleistung (Performance)**: Wie gut man eine Aufgabe *jetzt* löst
- **Lernen (Learning)**: Die Fähigkeit, ähnliche Aufgaben *später unabhängig* zu lösen

KI verbessert Aufgabenleistung (eindeutig belegt). Aber das sagt nichts über Lernen. Lernen erfordert möglicherweise genau die Anstrengung, die KI eliminiert.

### Die Bastani-Studie im Detail

Schauen wir uns die eingangs erwähnte Studie [@bastaniGenerativeAICan2024] genauer an:

**Design:**

- Rund 1000 türkische Gymnasiasten
- Randomisierte Zuweisung zu drei Gruppen
- Mathe-Übungen über mehrere Wochen
- Test am Ende ohne KI-Zugang

**Die Bedingungen:**

1. **Kontrollgruppe**: Kein KI-Zugang
2. **Direkter GPT-4-Zugang**: Freie Nutzung
3. **GPT Tutor**: Strukturierte Nutzung mit pädagogischen Leitplanken

**Die Ergebnisse:**

- Mit direktem GPT-4: **48% mehr Aufgaben gelöst**
- Mit GPT Tutor: **127% mehr Aufgaben gelöst**
- Ohne KI (später): **17% schlechter** als Kontrollgruppe

Die Autoren fassen zusammen:

> "Students attempt to use GPT-4 as a 'crutch' during practice sessions, and when successful, perform worse on their own."

**Wichtige Nuance:** Die negativen Effekte betrafen primär den direkten Zugang. Der "GPT Tutor" zeigte bessere Ergebnisse, aber selbst mit Tutor war die spätere Leistung ohne KI reduziert. Die Art der KI-Nutzung macht einen Unterschied.

**Einschränkungen:** Dies ist eine Einzelstudie, Replikation steht aus. Der Kontext (türkische Gymnasiasten, Mathematik) ist spezifisch. Die Effekte sind kurzfristig gemessen. Die Studie zeigt ein Muster, beweist aber nicht Universalität.

### Desirable Difficulties: Warum Anstrengung nötig ist

Robert Bjork [@bjorkMakingThingsHard2011] hat das Konzept der "erwünschten Schwierigkeiten" geprägt:

> "Conditions that slow the rate of apparent learning often optimize long-term retention and transfer."

Vier bewährte Interventionen illustrieren das Prinzip:

1. **Variation**: Lernen unter wechselnden Bedingungen
2. **Interleaving**: Aufgabentypen mischen statt blocken
3. **Spacing**: Verteilt lernen statt massiert
4. **Retrieval Practice**: Aktiv abrufen statt passiv wiederlesen

Alle vier haben gemeinsam: Sie fühlen sich *schwerer* an, sind aber *effektiver* für langfristiges Lernen.

**Der Mechanismus:** Sofortiger KI-Zugang kann Abrufversuche kurzschliessen. Statt selbst nachzudenken ("Was weiss ich darüber?"), fragt man die KI. Die Gedächtnisspur wird nicht gestärkt.

### Der Generierungseffekt

Der Generierungseffekt ist ein robuster Befund: Selbst generierte Information wird besser behalten als passiv erhaltene. Slamecka und Graf [-@slameckaGenerationEffect1978] demonstrierten dies erstmals experimentell, spätere Meta-Analysen bestätigten moderate Effektstärken.

Das typische Experiment:

- Gruppe A: Liest Wortpaare (Heiss — Kalt)
- Gruppe B: Ergänzt Wortpaare (Heiss — K___)
- Test: Beide Gruppen werden abgefragt
- Ergebnis: Gruppe B erinnert besser

**Warum funktioniert es?**

- Aktivere Verarbeitung während des Generierens
- Mehr Verbindungen im Gedächtnis
- Tiefere Encodierung

**Die Implikation:** Wenn KI generiert, was Studierende selbst produzieren sollten, wird der Generierungseffekt eliminiert. Studierende, die selbst einen Essay-Entwurf schreiben, profitieren vom Generierungseffekt. Studierende, die einen KI-generierten Entwurf bearbeiten, nicht, auch wenn das Endprodukt ähnlich aussieht.

### Die Scaffolding-Hypothese

Eine tiefere Frage drängt sich auf: Was passiert mit der Entwicklung kognitiver Fähigkeiten? Hier müssen wir unterscheiden:

**Fertigkeitsatrophie:** Eine vorhandene Fähigkeit verkümmert durch Nichtgebrauch. Das ist reversibel: Wenn man wieder übt, kommt die Fähigkeit zurück.

**Entwicklungsbeeinträchtigung:** Eine Fähigkeit entsteht nie, weil der konstruktive Prozess übersprungen wird. Das ist möglicherweise irreversibel: Das Entwicklungsfenster wurde verpasst.

Die Hypothese: Grundfertigkeiten wie Schreiben, Rechnen und analytisches Lesen sind nicht nur *Fertigkeiten*. Sie sind *Prozesse*, die kognitive Architektur aufbauen. Schreiben ist ein "epistemisches Werkzeug": Gedanken entwickeln sich *durch* das Schreiben, nicht vor dem Schreiben.

Wenn diese Prozesse übersprungen werden, entstehen höhere Fähigkeiten möglicherweise nicht.

### Historische Analogien

Frühere Technologien zeigen ähnliche Muster:

**GPS und räumliches Gedächtnis** [@dahmaniHabitualUseGPS2020]: Eine longitudinale Studie über drei Jahre zeigte: Stärkere GPS-Nutzung korreliert mit steilerem Rückgang des räumlichen Gedächtnisses. Die zeitliche Abfolge ist konsistent mit der Interpretation, dass GPS-Nutzung zum Rückgang beiträgt, wobei ungemessene konfundierende Variablen nicht ausgeschlossen werden können.

**Taschenrechner** [@sieglerDevelopingConceptualUnderstanding2017]: Die Forschung zeigt kontextabhängige Effekte. Für Schüler mit bestehenden Grundkenntnissen ist der Effekt neutral oder leicht positiv. Für Schüler ohne Grundkenntnisse ist der Effekt negativ. Dieselbe Parallele wie bei KI: Das Werkzeug wirkt unterschiedlich je nach Vorwissen.

**Google-Effekt** [@sparrowGoogleEffectsMemory2011]: Menschen erinnern Information schlechter, wenn sie erwarten, dass sie verfügbar bleibt. Sie erinnern stattdessen, *wo* die Information zu finden ist. Das Gedächtnis adaptiert sich an die Verfügbarkeit externer Speicher.

### Grenzen der Analogien

Die historischen Analogien sind suggestiv, aber nicht ausreichend:

- GPS betrifft räumliche Navigation
- Taschenrechner betreffen arithmetische Berechnungen
- Google betrifft Informationsabruf

Jede dieser Technologien betrifft eine **spezifische, enge** kognitive Funktion.

Generative KI kann fast **jede** kognitive Aufgabe übernehmen: Schreiben, Argumentieren, Analysieren, Synthetisieren, Bewerten. Die Breite ist beispiellos. Wir können nicht einfach extrapolieren.

**Aber:** Die historischen Bedenken hatten oft Berechtigung. GPS *beeinflusst* tatsächlich räumliche Kognition. Taschenrechner *veränderten* den Mathematikunterricht. Die Bedenken waren nicht blosse Panikmache.

### "Das haben sie über das Schreiben auch gesagt"

Ein häufiger Einwand lautet: Sokrates warnte vor der Schrift, und wir haben überlebt. Jede neue Technologie löst Panik aus.

Im Phaidros warnte Sokrates: Schrift wird das Gedächtnis schwächen und nur "Scheinwissen" erzeugen.

Drei Antworten:

1. **Schrift *hat* Kognition tiefgreifend verändert.** Wir denken anders als orale Kulturen. Abstraktion, Kategorisierung, lineare Argumentation wurden durch Schrift gefördert. Das war nicht nur positiv oder negativ, es war transformativ.

2. **Einige Bedenken waren berechtigt.** Mündliche Gedächtnistraditionen *sind* zurückgegangen. Homers Epen wurden über Generationen mündlich überliefert. Diese Fähigkeit ist weitgehend verloren.

3. **Schriftkultur entwickelte sich über Jahrhunderte.** Es gab Zeit für kulturelle Anpassung. Bildungssysteme entwickelten sich mit. KI-Integration geschieht in Jahren, nicht Jahrhunderten.

### Warum Experten profitieren, Lernende nicht

Jetzt können wir das Muster erklären:

**Experten können:**

- Routine-Aufgaben sicher auslagern (sie wissen, was "Routine" ist)
- Höheres Denken aufrechterhalten (kognitive Kapazität wird frei)
- KI-Outputs bewerten (sie haben Domänenexpertise)
- Ihre Grundfähigkeiten verkümmern nicht (sie sind schon da)

**Lernenden fehlt:**

- Wissen zur Bewertung (sie können nicht einschätzen, ob KI richtig liegt)
- Etablierte Grundfähigkeiten (was nicht da ist, kann nicht verkümmern, entsteht aber auch nicht)
- Metakognitive Kontrolle (sie wissen nicht, wann KI-Nutzung schadet)

Das Ergebnis kann **"fliessende Inkompetenz"** sein: Anspruchsvoll wirkende Outputs ohne zugrundeliegendes Verständnis. Das KI-generierte Produkt sieht kompetent aus, das Wissen fehlt.

**Dasselbe Werkzeug, fundamental unterschiedliche Auswirkungen.**

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Lernsituationen gestalten

Um produktive Anstrengung zu erhalten:

1. **"Ohne-KI"-Phasen einplanen**: Explizite Zeiten, in denen KI nicht erlaubt ist
2. **Prozess bewerten, nicht nur Produkt**: Zwischenschritte einfordern und bewerten
3. **Retrieval Practice integrieren**: Regelmässige Abrufübungen ohne Hilfsmittel
4. **Spacing und Interleaving nutzen**: Verteiltes, gemischtes Üben
5. **Generierung fordern**: Eigene Entwürfe vor KI-Unterstützung verlangen
6. **Reflexion einbauen**: Studierende über ihren Lernprozess nachdenken lassen
:::

## Exkurs: Sokratisches Fragen in KI-Tutoren {#sokrates}

Ein konkretes Beispiel für den Hype-Evidenz-Gap: Viele EdTech-Unternehmen bewerben ihre KI-Tutoren mit "sokratischer Methode".

### Das Versprechen

Die Argumentation ist verlockend:

- Blooms "Two Sigma Problem" [@bloom2SigmaProblem1984]: 1:1-Tutoring erzielt 2 Standardabweichungen Verbesserung
- Das würde einen durchschnittlichen Studierenden in die Top 2% bringen
- KI kann unbegrenzt viele Lernende betreuen
- Also: Demokratisierung personalisierter Bildung

**Aber:** Die Begeisterung übersteigt die Evidenz erheblich.

### Warum Fragen theoretisch helfen könnten

Die kognitionswissenschaftliche Grundlage ist solide:

**Generierungseffekt:** Selbst erzeugte Antworten werden besser behalten als passiv erhaltene [@slameckaGenerationEffect1978].

**Selbsterklärungseffekt:** Erklären fördert tiefere Verarbeitung und deckt Wissenslücken auf [@chiElicitingSelfExplanationsImproves1994].

**Aber:** Die Qualität der Selbsterklärungen und damit der Lerneffekt hängt vom Vorwissen ab. Das verknüpft mit dem Expertise-Umkehr-Effekt: Was für Fortgeschrittene funktioniert, kann Novizen überfordern.

### Was die Evidenz tatsächlich zeigt

VanLehn [-@vanlehnRelativeEffectivenessHuman2011] führte eine Meta-Analyse zur Effektivität verschiedener Tutoring-Formen durch:

- **Menschliche Tutoren:** d = 0.79 (nicht 2.0 wie Bloom behauptete)
- **Intelligente Tutorsysteme:** d = 0.76 (vergleichbar)

VanLehns Analyse legt nahe, dass Schritt-für-Schritt-Feedback ein wesentlicher Faktor war, wobei die genaue Rolle des sokratischen Dialogs schwer zu isolieren ist.

**Zu LLM-basierten sokratischen Tutoren:** Es gibt keine gut kontrollierten randomisierten Studien. Die existierende Evidenz besteht hauptsächlich aus Zufriedenheitsumfragen und Vergleichen mit "kein Tutoring" statt mit Alternativen.

### Das Diagnose-Problem

Effektives sokratisches Fragen erfordert:

- Genaue Einschätzung des aktuellen Wissensstands
- Unterscheidung verschiedener Fehlertypen
- Anpassung der Fragen an den individuellen Lernenden

**Beispiel:** Wenn ein Schüler antwortet "1/2 + 1/3 = 2/5", kann das bedeuten:

- Prozeduraler Fehler (Zähler und Nenner addiert)
- Konzeptueller Fehler (versteht nicht, was Brüche repräsentieren)
- Flüchtigkeitsfehler (weiss es eigentlich)

Die angemessene sokratische Frage unterscheidet sich je nach Ursache. Menschliche Tutoren nutzen Mimik, Tonfall, Zögern und jahrelange Erfahrung zur Diagnose. KI-Systeme haben nur den Text und können diese Unterscheidung nicht zuverlässig treffen.

### Weitere Herausforderungen

**Fragesequenzierung:** Sokratischer Dialog ist kontingent. Jede Frage hängt von der vorherigen Antwort ab. LLMs generieren Token für Token, sie haben keinen pädagogischen Gesamtplan.

**Feedback-Timing:** Wann korrigieren, wann weiter fragen lassen? Zu früh verhindert eigenes Denken, zu spät frustriert und verfestigt Fehler. Die Balance hängt vom individuellen Lernenden ab.

**LLM-Sycophancy:** LLMs sind darauf trainiert, hilfreich und angenehm zu sein. Sie tendieren dazu, Nutzern zuzustimmen. Das ist das Gegenteil von produktivem sokratischem Unbehagen. Sokrates machte seine Gesprächspartner unbequem. Das war der Punkt.

### Fazit zum sokratischen KI-Tutoring

**Die ehrliche Antwort ist: Wir wissen es noch nicht.**

*Was plausibel ist:* Selbsterklärung und Generierung fördern Lernen. Fragen können diese Prozesse anregen.

*Was nicht belegt ist:* Dass aktuelle KI-Systeme die nötige Diagnose leisten können. Dass LLM-basierte sokratische Tutoren besser sind als Alternativen. Dass positive Effekte langfristig halten.

Sokrates würde es schätzen: Die beste Art, Werkzeuge zu bewerten, die seine Methode beanspruchen, ist, kritische Fragen zu stellen und unbegründete Antworten nicht zu akzeptieren.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: KI-Tutoren evaluieren

Bei der Evaluation von KI-Tutoring-Tools:

1. **Evidenz verlangen**: Peer-Review-Studien mit Lernoutcomes, nicht nur Zufriedenheit
2. **Diagnose-Fähigkeit prüfen**: Kann das System zwischen Fehlertypen unterscheiden?
3. **Opportunitätskosten bedenken**: Ist KI-Tutoring besser als Lehrbuch, Übungsaufgaben, Peer-Diskussion?
4. **Mit strukturierten Domänen beginnen**: Mathematik vor Literaturanalyse
5. **Auf unbeabsichtigte Folgen achten**: Gaming, Abhängigkeit von externen Prompts
6. **Pilotieren und messen**: Kleine Versuche mit klaren Erfolgskriterien
:::

## Implikationen und offene Fragen {#implikationen}

### Die unbequeme Wahrheit

Was bedeutet das alles? Die zentrale These lässt sich nun präzisieren:

> **Was KI für Produktivität nützlich macht, macht sie für Lernen schädlich: Sofortige Antworten eliminieren die Anstrengung, die Kompetenz aufbaut.**

Dies ist kein Mangel aktueller KI. Es folgt aus etablierten Prinzipien der Kognitionswissenschaft. Bessere KI wird das Problem nicht lösen, weil das Problem in der Natur des Lernens liegt.

**Einschränkung:** Die direkte Evidenz für KI ist noch begrenzt. Die theoretische Argumentation ist stark, aber Langzeitstudien fehlen.

### Mind-extending vs. Mind-replacing

Andy Clark [@clarkExtendingMindsGenerative2025], bekannt für die "Extended Mind Thesis", unterscheidet zwei Nutzungsweisen:

**Kognition erweitern:**

- Der Mensch bleibt kognitiv engagiert
- Werkzeug verstärkt, ersetzt nicht
- Beispiel: Taschenrechner für einen Mathematiker
- Fähigkeiten bleiben erhalten und werden ausgebaut

**Kognition ersetzen:**

- Der Mensch wird passiv
- Werkzeug übernimmt das Denken
- Beispiel: KI schreibt den Essay, Studierender submittet
- Abhängigkeit entsteht, Fähigkeiten verkümmern

**Dasselbe Werkzeug kann beides sein, abhängig von der Nutzung.** Die Frage ist nicht "KI ja oder nein?", sondern "Wie wird KI genutzt?"

### Die Sequenzierungsfrage

Was bedeutet das praktisch? Die Forschung legt nahe:

- Studierende brauchen wahrscheinlich **Grundwissen, bevor** KI vorteilhaft wird
- Der Expertise-Umkehr-Effekt empfiehlt **dynamische** Policies
- Die Schwelle, ab der KI von schädlich zu hilfreich wechselt, ist **unbekannt**
- Die Antwort ist vermutlich **domänen- und studentenspezifisch**

Es gibt keine universelle Regel. Lehrende müssen für ihre Kontexte entscheiden.

### Die Entwicklungsfrage

Ein besonderes Anliegen betrifft die kognitive Entwicklung:

- Der präfrontale Kortex entwickelt sich bis etwa Mitte 20, wobei verschiedene Funktionen unterschiedlich schnell reifen und individuelle Variation erheblich ist
- Exekutive Funktionen, Metakognition, Selbstregulation sind bei vielen Studierenden noch in Entwicklung
- **Paradox:** Jene, die KI-Nutzung am wenigsten regulieren können, sind möglicherweise am verletzlichsten

Die aktuelle Studierendengeneration könnte die erste sein, die ihre gesamte Bildungslaufbahn mit generativer KI durchläuft. Wenn wir in 20 Jahren Langzeitdaten haben, wird eine Generation bereits das Experiment gewesen sein.

### Die Equity-Dimension

Die "dritte digitale Kluft" [@trucanoThirdDigitalDivide2024] beschreibt ein neues Ungleichheitsmuster:

| Erste Kluft | Zweite Kluft | Dritte Kluft |
|-------------|--------------|--------------|
| Zugang zu Geräten | Fähigkeit zur sinnvollen Nutzung | Qualität der pädagogischen Integration |

Die Ironie: "Demokratisierung" durch KI könnte Ungleichheit verstärken:

- Gutausgestattete Studierende: ausgeklügelte pädagogische Unterstützung, informierte Betreuung, strukturierte KI-Nutzung
- Unterversorgte Studierende: KI als unbeaufsichtigte Abkürzung, niemand erklärt die Risiken

### Der stärkste Gegeneinwand

Der stärkste Einwand lautet: "Wenn KI immer verfügbar ist, müssen Fähigkeiten nicht internalisiert werden."

Vier Antworten:

1. **Permanenzannahme:** Setzt voraus, dass KI immer verfügbar, funktional und bezahlbar bleibt. Stromausfall, Serverprobleme, Kosten, politische Entscheidungen können das ändern.

2. **Rekursionsproblem:** Wer erkennt, wenn KI falsch liegt? Wer trainiert die nächste KI-Generation? Wer erweitert menschliches Wissen? Irgendwer muss Domänenexpertise haben.

3. **Autonomie-Argument:** Eigenständiges Denken hat intrinsischen Wert für Selbstbestimmung, Würde, das Gefühl des Verstehens. Nicht alles lässt sich in Produktivität messen.

4. **Unbekannte Unbekannte:** Komplexe Systeme haben Kaskadeneffekte. Wir wissen nicht, was wir verlieren könnten.

### Was wir noch nicht wissen

Epistemische Bescheidenheit ist angebracht. Wir wissen vieles nicht:

- **Längsschnittstudien über Jahre:** Praktisch nicht vorhanden
- **Transfer auf neue Kontexte:** Unerforscht
- **Optimale Scaffolding-Bedingungen:** Unbekannt
- **Disziplinspezifische Effekte:** Untererforscht
- **Publikationsbias:** Wahrscheinlich vorhanden

### Abschliessende Überlegungen

Drei Zitate fassen die Lage zusammen:

> "If AI assistance during education impairs independent capability, students may graduate less prepared for contexts where AI is unavailable."

> "The productivity gains during education would come at the cost of capability thereafter."

> "Whether this tradeoff is acceptable depends on assumptions about the future that educators cannot verify."

Eine letzte Asymmetrie verdient Beachtung: Wenn wir uns bei den Risiken irren, haben wir die Einführung nützlicher Technologie etwas verlangsamt. Wenn sich die Vorsichtigen bei der Sicherheit irren, haben wir möglicherweise die kognitive Entwicklung einer Generation beeinträchtigt.

::: {.pro-tip collapse="true"}
## Praktische Empfehlung: Entscheidungsrahmen

Für Entscheidungen über KI in der eigenen Lehre:

**Fragen zur Selbstreflexion:**

1. Welche kognitiven Prozesse will ich fördern?
2. Welche davon könnte KI übernehmen?
3. Ist die Übernahme für Lernen förderlich oder hinderlich?
4. Haben meine Studierenden das nötige Vorwissen für kritische KI-Nutzung?
5. Wie kann ich Grundlagen schützen und dennoch KI sinnvoll einsetzen?

**Prinzipien:**

- Grundlagen vor Werkzeugen
- Prozess bewerten, nicht nur Produkt
- Differenzieren nach Vorwissen
- Pilotieren und messen
- Transparent kommunizieren
- Flexibel anpassen

**Haltung:**

- Weder Panik noch unkritische Begeisterung
- Evidenzbasiert, aber handlungsfähig
- Epistemische Bescheidenheit bei begründeter Vorsicht
:::

## Fazit {#fazit}

Die zentrale Botschaft: KI-Werkzeuge sind für Experten konzipiert. Sie machen Experten produktiver, während Lernende oft nicht profitieren, weil Lernen die kognitive Anstrengung erfordert, die KI eliminiert.

Die Argumentation stützt sich auf:

- **Cognitive Load Theory**: Lernen erfordert produktive Anstrengung durch das Nadelöhr des Arbeitsgedächtnisses
- **Expertise-Umkehr-Effekt**: Dieselbe Unterstützung kann Novizen helfen und Experten schaden
- **Domänenspezifität**: Kritische KI-Bewertung erfordert Fachwissen, nicht nur generische Strategien
- **Desirable Difficulties**: Schwierigkeiten, die das Lernen verlangsamen, optimieren oft Langzeitbehalten
- **Generierungseffekt**: Selbst erzeugte Information wird besser behalten

Die praktischen Implikationen sind:

- Grundlagen vor Werkzeugen sequenzieren
- Prozess bewerten, nicht nur Produkt
- Nach Vorwissen differenzieren
- "Ohne-KI"-Phasen einplanen
- Kritische KI-Nutzung *im Fachkontext* üben

Die offenen Fragen sind zahlreich. Langzeitstudien fehlen, optimale Strategien sind unbekannt, und die Effekte variieren nach Kontext und Person.

Was bleibt, ist eine Haltung: **Informierte Vorsicht bei Handlungsfähigkeit.** Weder Panik noch unkritische Begeisterung. Evidenz ernst nehmen, auch wenn sie unvollständig ist. Die Verantwortung für Bildungsentscheidungen nicht an Technologietrends delegieren.

Die Frage ist nicht, ob KI in die Bildung kommt. Sie ist schon da. Die Frage ist, wie wir sie so gestalten, dass sie dem Lernen dient, nicht es ersetzt.

## Referenzen {.unnumbered}

::: {#refs}
:::
